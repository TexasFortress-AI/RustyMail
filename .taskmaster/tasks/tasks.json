{
  "master": {
    "tasks": [
      {
        "id": "1",
        "title": "Create database migration for AI model configurations",
        "description": "Add ai_model_configurations table to store tool-calling and drafting model settings",
        "details": "Create migrations/004_create_ai_model_config.sql with table schema for role, provider, model_name, base_url, api_key, additional_config. Include default entries for qwen2.5:7b (tool-calling) and llama3.3:70b (drafting)",
        "testStrategy": "",
        "status": "done",
        "dependencies": [],
        "priority": "high",
        "subtasks": [],
        "complexity": 4,
        "recommendedSubtasks": 5,
        "expansionPrompt": "Break down the systematic removal of imap-types dependency: 1) Remove from Cargo.toml, 2) Find and eliminate all imap-types imports across 80+ Rust files, 3) Replace imap-types types with async-imap equivalents in client.rs and session.rs, 4) Update type conversions and error handling, 5) Verify compilation succeeds"
      },
      {
        "id": "2",
        "title": "Create model configuration service module",
        "description": "Implement model_config.rs for managing AI model configurations in database",
        "details": "Create src/dashboard/services/ai/model_config.rs with ModelConfiguration struct, get_model_config(), set_model_config(), and database CRUD operations. Support both 'tool_calling' and 'drafting' roles.",
        "testStrategy": "",
        "status": "done",
        "dependencies": [
          "1"
        ],
        "priority": "high",
        "subtasks": [],
        "complexity": 3,
        "recommendedSubtasks": 4,
        "expansionPrompt": "Address specific compilation errors: 1) Fix missing lifetime specifier in api/sse.rs Context type around line 80-90, 2) Add missing validate_api_key function in api/rest.rs (referenced but not implemented), 3) Add missing Mutex import in imap/session.rs, 4) Run cargo check to verify all fixes"
      },
      {
        "id": "3",
        "title": "Create high-level tools definitions module",
        "description": "Implement high_level_tools.rs with tool definitions and routing",
        "details": "Create src/dashboard/api/high_level_tools.rs with get_mcp_high_level_tools_jsonrpc_format() returning 10-12 tool definitions: process_email_instructions, draft_reply, draft_email, list_accounts, list_folders_hierarchical, list_cached_emails, get_email_by_uid, search_cached_emails, get_folder_stats, get_model_configurations, set_tool_calling_model, set_drafting_model. Include execute_high_level_tool() router function.",
        "testStrategy": "",
        "status": "done",
        "dependencies": [
          "2"
        ],
        "priority": "high",
        "subtasks": [],
        "complexity": 6,
        "recommendedSubtasks": 7,
        "expansionPrompt": "Refactor IMAP client architecture: 1) Update client.rs to use only async-imap types, 2) Remove dual-library type conversion functions, 3) Standardize AsyncImapSessionWrapper pattern in session.rs, 4) Update connection handling in client.rs lines 90-155, 5) Map async-imap errors to domain errors, 6) Update all IMAP operations to use async-imap exclusively, 7) Test connection lifecycle and operations"
      },
      {
        "id": "4",
        "title": "Implement model configuration MCP tools",
        "description": "Create handlers for get_model_configurations, set_tool_calling_model, set_drafting_model",
        "details": "Implement the three configuration tools that read/write to ai_model_configurations table. Validate model configurations and test connectivity before saving.",
        "testStrategy": "",
        "status": "done",
        "dependencies": [
          "2",
          "3"
        ],
        "priority": "high",
        "subtasks": [],
        "complexity": 7,
        "recommendedSubtasks": 8,
        "expansionPrompt": "Design atomic IMAP command sequences: 1) Research IMAP atomic operation patterns, 2) Implement SELECT-COPY-STORE-EXPUNGE sequence for moves, 3) Add transaction-like error handling with rollback, 4) Track folder selection state in AsyncImapSessionWrapper, 5) Implement operation state management, 6) Add concurrent access protection, 7) Create atomic operation tests, 8) Verify ACID properties in error scenarios"
      },
      {
        "id": "5",
        "title": "Wire up browsing tools to high-level variant",
        "description": "Connect existing read-only browsing tools to high-level tool router",
        "details": "Reuse existing handlers for list_accounts, list_folders_hierarchical, list_cached_emails, get_email_by_uid, search_cached_emails, get_folder_stats. Add routing logic in execute_high_level_tool() to call these handlers.",
        "testStrategy": "",
        "status": "done",
        "dependencies": [
          "3"
        ],
        "priority": "medium",
        "subtasks": [],
        "complexity": 5,
        "recommendedSubtasks": 6,
        "expansionPrompt": "Implement comprehensive error system: 1) Define error code ranges (-32700 to -32099) following JSON-RPC 2.0 spec, 2) Create error mapping from async-imap errors to JSON-RPC codes, 3) Implement consistent error response format across REST/MCP interfaces, 4) Add structured error details with operation context, 5) Update existing error handling in rest.rs and mcp modules, 6) Test error propagation through all interface layers"
      },
      {
        "id": "6",
        "title": "Create email drafter service",
        "description": "Implement email_drafter.rs for generating email drafts using configured model",
        "details": "Create src/dashboard/services/ai/email_drafter.rs with EmailDrafter struct, draft_reply() and draft_email() methods. Use model_config to get drafting model settings, call Ollama API to generate text. Include context from original email for replies.",
        "testStrategy": "",
        "status": "done",
        "dependencies": [
          "2"
        ],
        "priority": "high",
        "subtasks": [],
        "complexity": 8,
        "recommendedSubtasks": 10,
        "expansionPrompt": "Implement comprehensive IMAP operations: 1) Complete folder listing with hierarchical structure, 2) Implement server-side search using async-imap search methods, 3) Add message fetching with MIME part handling, 4) Complete atomic move operations with guarantees, 5) Implement delete operations with proper flag handling, 6) Add EXPUNGE operations, 7) Create integration tests for each operation, 8) Test with multiple IMAP server types (Gmail, Outlook), 9) Optimize for performance requirements (<500ms folder list, <200ms email fetch), 10) Add comprehensive error handling for each operation"
      },
      {
        "id": "7",
        "title": "Implement draft_reply and draft_email tools",
        "description": "Create MCP tool handlers for email drafting",
        "details": "Implement handlers that fetch email content, construct prompts, call EmailDrafter service, and return formatted draft text. Handle errors gracefully.",
        "testStrategy": "",
        "status": "done",
        "dependencies": [
          "6"
        ],
        "priority": "medium",
        "subtasks": [],
        "complexity": 7,
        "recommendedSubtasks": 8,
        "expansionPrompt": "Build robust connection management: 1) Design connection pool using Arc<TokioMutex<>> pattern, 2) Implement session lifecycle tracking and cleanup, 3) Add connection health checking mechanisms, 4) Implement automatic reconnection logic, 5) Add session timeout handling, 6) Support concurrent session handling up to 100+ connections, 7) Create connection leak prevention, 8) Add performance monitoring and load testing for concurrent connections"
      },
      {
        "id": "8",
        "title": "Create MCP to Ollama tool converter",
        "description": "Implement tool_converter.rs for converting MCP tool schemas to Ollama format",
        "details": "Create src/dashboard/services/ai/tool_converter.rs with mcp_to_ollama_tools() function. Convert MCP JSON Schema inputSchema to Ollama's tool format. Handle type conversions and required fields.",
        "testStrategy": "",
        "status": "done",
        "dependencies": [],
        "priority": "high",
        "subtasks": [],
        "complexity": 6,
        "recommendedSubtasks": 7,
        "expansionPrompt": "Replace custom MCP implementation with official SDK: 1) Remove custom MCP transport implementations in legacy.rs, 2) Integrate rmcp dependency from rust-sdk (already in Cargo.toml), 3) Study official SDK patterns and API, 4) Update service definitions using SDK tooling, 5) Migrate stdio transport to use SDK patterns, 6) Migrate SSE transport to use SDK patterns, 7) Ensure backward compatibility with existing MCP clients and test integration"
      },
      {
        "id": "9",
        "title": "Create agent executor with Ollama tool calling",
        "description": "Implement agent_executor.rs for running sub-agent with iterative tool calling",
        "details": "Create src/dashboard/services/ai/agent_executor.rs with AgentExecutor struct and execute_with_tools() method. Implement iterative loop: send instruction with tools to Ollama, handle tool_calls response, execute requested tools using existing handlers, send results back, repeat until completion. Aggregate results and return formatted response with actions_taken list.",
        "testStrategy": "",
        "status": "done",
        "dependencies": [
          "8"
        ],
        "priority": "high",
        "subtasks": [],
        "complexity": 6,
        "recommendedSubtasks": 8,
        "expansionPrompt": "Finish REST API development: 1) Complete remaining REST endpoints for folder operations, 2) Implement email search, fetch, move, and delete endpoints, 3) Add API key authentication middleware (validate_api_key function missing), 4) Add request validation and rate limiting, 5) Implement proper HTTP status codes following REST conventions, 6) Create comprehensive OpenAPI/Swagger documentation, 7) Add end-to-end tests for all endpoints, 8) Performance test under load conditions"
      },
      {
        "id": "10",
        "title": "Implement process_email_instructions tool",
        "description": "Create MCP tool handler for complex email workflow execution",
        "details": "Implement handler that takes natural language instruction, gets tool-calling model config, converts all low-level MCP tools to Ollama format, calls AgentExecutor, formats result. Include logic to detect when user feedback is needed and return questions in JSON format.",
        "testStrategy": "",
        "status": "done",
        "dependencies": [
          "9"
        ],
        "priority": "high",
        "subtasks": [],
        "complexity": 5,
        "recommendedSubtasks": 6,
        "expansionPrompt": "Complete dashboard backend functionality: 1) Complete metrics collection service using sysinfo crate, 2) Implement client management service for tracking active connections, 3) Add configuration service for runtime settings, 4) Create SSE event broadcasting system for real-time updates, 5) Implement system health monitoring, 6) Test metrics accuracy and SSE broadcasting with multiple clients"
      },
      {
        "id": "11",
        "title": "Add high-level variant support to MCP HTTP backend",
        "description": "Modify mcp_http.rs to support ?variant=high-level query parameter",
        "details": "Update tools/list handler to check for variant parameter and return high-level tools when variant=high-level. Update tools/call handler to route to execute_high_level_tool() for high-level variant. Store variant in session data.",
        "testStrategy": "",
        "status": "done",
        "dependencies": [
          "3",
          "10"
        ],
        "priority": "high",
        "subtasks": [],
        "complexity": 4,
        "recommendedSubtasks": 5,
        "expansionPrompt": "Complete frontend integration: 1) Build React frontend using Vite in frontend/rustymail-app-main/ directory, 2) Configure build output for production, 3) Integrate static files with Actix backend using actix-files middleware, 4) Configure SSE EventSource connections for real-time updates, 5) Test frontend build process and backend integration across different browsers"
      },
      {
        "id": "12",
        "title": "Create high-level MCP stdio binary",
        "description": "Create rustymail-mcp-stdio-high-level binary",
        "details": "Create src/bin/mcp_stdio_high_level.rs that connects to backend with ?variant=high-level parameter. Can be copy of mcp_stdio.rs with modified default URL, or same binary with --mode flag. Add binary to Cargo.toml.",
        "testStrategy": "",
        "status": "done",
        "dependencies": [
          "11"
        ],
        "priority": "medium",
        "subtasks": [],
        "complexity": 5,
        "recommendedSubtasks": 6,
        "expansionPrompt": "Complete SSE implementation: 1) Finish SSE implementation in dashboard/api/sse.rs, 2) Add event broadcasting for metrics updates and client connections, 3) Implement proper SSE connection lifecycle management, 4) Add event filtering and subscription management, 5) Ensure browser reconnection handling, 6) Test SSE stability under network conditions and with multiple concurrent clients"
      },
      {
        "id": "13",
        "title": "Test high-level MCP variant with Claude Desktop",
        "description": "Integration testing of complete high-level tool flow",
        "status": "deferred",
        "dependencies": [
          "12"
        ],
        "priority": "medium",
        "details": "Configure Claude Desktop to use rustymail-mcp-stdio-high-level binary. Migration 004 (ai_model_configurations table) has been applied with default models: qwen2.5:7b for tool_calling and llama3.3:70b for drafting. Test workflow: 1) Use set_tool_calling_model and set_drafting_model tools to configure actual models instead of defaults, 2) Test browsing tools (list_accounts, list_folders_hierarchical, get_email_by_uid), 3) Test configuration tools (get_model_configurations), 4) Test drafting tools (draft_reply, draft_email), 5) Test process_email_instructions with simple workflows. Verify tool count is ~12 instead of 26+. Process_email_instructions tool should now work properly after database migration fix.",
        "testStrategy": "Configure Claude Desktop with rustymail-mcp-stdio-high-level, verify 12 tools available instead of 26+. First configure models using configuration tools, then test each tool category: browsing (list accounts/folders/emails), drafting (generate replies/emails), and workflow execution (process_email_instructions). Confirm all tools work without database errors.",
        "subtasks": [
          {
            "id": 1,
            "title": "Configure Claude Desktop with high-level MCP binary",
            "description": "Set up Claude Desktop to use rustymail-mcp-stdio-high-level binary and verify connection",
            "dependencies": [],
            "details": "Update Claude Desktop configuration to point to target/release/rustymail-mcp-stdio-high-level binary. Ensure server is running on configured port. Verify Claude Desktop shows ~12 tools available instead of 26+.",
            "status": "pending",
            "testStrategy": "Check Claude Desktop shows correct tool count and can connect to MCP server",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Configure AI models using configuration tools",
            "description": "Use set_tool_calling_model and set_drafting_model to replace default configurations",
            "dependencies": [
              1
            ],
            "details": "Migration 004 created default configurations (qwen2.5:7b for tool_calling, llama3.3:70b for drafting). Use the MCP configuration tools to set actual models the user wants to use. Test get_model_configurations to verify settings are saved correctly.",
            "status": "pending",
            "testStrategy": "Verify model configurations are saved and retrieved correctly from ai_model_configurations table",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Test browsing tools functionality",
            "description": "Test list_accounts, list_folders_hierarchical, list_cached_emails, and get_email_by_uid tools",
            "dependencies": [
              2
            ],
            "details": "Verify all browsing tools work correctly with high-level MCP variant. Test that these tools provide the same functionality as the low-level variant but through the simplified interface.",
            "status": "pending",
            "testStrategy": "Execute each browsing tool and verify expected data structure and content returned",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Test drafting tools functionality",
            "description": "Test draft_reply and draft_email tools using configured drafting model",
            "dependencies": [
              2
            ],
            "details": "Verify AI-powered drafting tools work with the configured drafting model from step 2. Test that drafts are generated with appropriate quality and relevance to input context.",
            "status": "pending",
            "testStrategy": "Generate sample drafts and verify they are contextually appropriate and well-formatted",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Test process_email_instructions workflow execution",
            "description": "Test the main workflow tool with simple email management instructions",
            "dependencies": [
              2,
              3
            ],
            "details": "Test process_email_instructions with simple workflows like 'list unread emails in INBOX' or 'show folder statistics'. Should now work correctly after ai_model_configurations table migration fix. Verify the tool uses other available tools to complete the workflow.",
            "status": "pending",
            "testStrategy": "Execute simple instructions and verify workflow completion without database errors",
            "parentId": "undefined"
          }
        ],
        "complexity": 3,
        "recommendedSubtasks": 4,
        "expansionPrompt": "Initialize AI integration framework: 1) Add RIG dependency to Cargo.toml, 2) Configure OpenAI and OpenRouter providers in dashboard/services/ai/ modules, 3) Set up LLM model selection and API key management, 4) Create provider abstraction layer and test connectivity with configured providers",
        "updatedAt": "2026-01-23T11:46:31.369Z"
      },
      {
        "id": "14",
        "title": "Create WebUI settings page for AI model configuration",
        "description": "Add /settings/ai-models page to dashboard",
        "details": "Create UI for configuring tool-calling and drafting models. Include provider dropdown, model name input with autocomplete, base URL input, API key input, test connection button, save button. Display current configurations. Wire up to backend API endpoints.",
        "testStrategy": "",
        "status": "done",
        "dependencies": [
          "4"
        ],
        "priority": "low",
        "subtasks": [],
        "complexity": 6,
        "recommendedSubtasks": 7,
        "expansionPrompt": "Build NLP to MCP pipeline: 1) Design natural language processing pipeline using RIG, 2) Create prompt templates for common email operations ('show unread emails', 'emails from sender', 'move emails to folder'), 3) Implement intent recognition and mapping to MCP method calls, 4) Add conversation context tracking for follow-up queries, 5) Create query parsing and validation, 6) Test natural language understanding with various query formats, 7) Verify correct MCP operation mapping and context maintenance",
        "updatedAt": "2026-01-24T14:18:15.055Z"
      },
      {
        "id": "15",
        "title": "Document high-level MCP variant in README",
        "description": "Add documentation for new high-level variant",
        "details": "Update README with: explanation of two variants, configuration examples for both, tool list comparison, when to use each variant, model configuration instructions.",
        "testStrategy": "",
        "status": "done",
        "dependencies": [
          "13"
        ],
        "priority": "low",
        "subtasks": [],
        "complexity": 4,
        "recommendedSubtasks": 5,
        "expansionPrompt": "Complete chatbot interface: 1) Extend existing ChatbotPanel.tsx component with full conversation interface, 2) Add message history and conversation state management, 3) Implement typing indicators and response streaming, 4) Connect to backend AI service via SSE or WebSocket, 5) Add conversation export and history features, and test UI responsiveness"
      },
      {
        "id": "16",
        "title": "Fix compose dialog appearing on hard refresh",
        "description": "Prevent the SendMailDialog from automatically opening when the web UI is hard-refreshed (F5 or Ctrl+F5)",
        "status": "done",
        "dependencies": [
          "4"
        ],
        "priority": "medium",
        "details": "SPECIFIC BUG IDENTIFIED: The dialog is appearing on page load with `data-state=\"open\"` in the DOM. The `composeDialogOpen` state is correctly initialized as `false` in EmailList.tsx:71, but something is triggering it to become `true` during component mount. Debug logging has been added at EmailList.tsx:74-76 to track state changes. The visual confusion is compounded by placeholder text in input fields (recipient@example.com, cc@example.com, etc.) that appears gray but looks like actual values. ROOT CAUSE INVESTIGATION NEEDED: 1) Trace what's calling setComposeDialogOpen(true) during initialization - check browser dev tools console for debug logs, 2) Verify if any useEffect hooks or props are triggering dialog open on mount, 3) Check if Radix UI Dialog component has any default behavior causing auto-open, 4) Investigate if URL parameters, localStorage, or sessionStorage are influencing initial state, 5) Ensure the Dialog component's `open` prop is properly controlled by the `composeDialogOpen` state variable",
        "testStrategy": "Test by: 1) Adding more granular debug logging to track exactly when and why setComposeDialogOpen(true) is called, 2) Performing a hard refresh (F5 or Ctrl+F5) and checking browser console for debug output, 3) Verifying dialog does not appear on hard refresh after fix, 4) Testing soft refresh and normal navigation to ensure functionality still works, 5) Testing actual compose dialog triggers (Compose button, Reply, Forward) to ensure they still work correctly, 6) Cross-browser testing in Chrome, Firefox, Safari to ensure consistent behavior, 7) Verify placeholder text styling doesn't create visual confusion about empty vs filled fields",
        "subtasks": [
          {
            "id": 1,
            "title": "Add comprehensive debug logging to track dialog state changes",
            "description": "Implement detailed logging to identify what triggers setComposeDialogOpen(true) during component initialization",
            "dependencies": [],
            "details": "Add console.log statements at all locations where setComposeDialogOpen is called, including stack traces. Log component mount/unmount cycles and prop changes. Add logging in useEffect hooks that might influence dialog state. Check EmailList.tsx:162 (handleComposeRequest) and EmailList.tsx:590 (Compose button click) for unexpected calls.",
            "status": "done",
            "testStrategy": "Open browser dev tools console, perform hard refresh, and verify detailed logs show exact sequence of state changes and what triggers dialog opening",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Investigate Radix UI Dialog component behavior on mount",
            "description": "Check if Radix UI Dialog has any default open behavior or hydration issues",
            "dependencies": [
              1
            ],
            "details": "Examine the Dialog component in components/ui/dialog.tsx and its usage in SendMailDialog.tsx:211. Verify the `open` prop is properly bound to composeDialogOpen state. Check if DialogPrimitive.Root has any default state that could cause auto-opening. Review Radix UI documentation for known hydration or SSR issues that might cause initial open state.",
            "status": "done",
            "testStrategy": "Test with different initial values for the open prop and verify the Dialog component respects the controlled state properly",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Add hard refresh detection to prevent unwanted dialog opening",
            "description": "Implement logic to detect hard refresh and ensure dialog remains closed",
            "dependencies": [
              1,
              2
            ],
            "details": "Add a useEffect hook in EmailList component that detects if the page was loaded fresh (hard refresh) vs navigated to. Use performance.navigation.type or window.performance.getEntriesByType('navigation') to detect refresh. Set a flag to prevent dialog from opening on fresh page loads. Ensure this doesn't interfere with legitimate compose dialog triggers.",
            "status": "done",
            "testStrategy": "Test hard refresh (F5/Ctrl+F5) vs normal navigation and verify dialog only opens when explicitly triggered by user actions",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Fix placeholder text styling to reduce visual confusion",
            "description": "Update input placeholder styling to be more clearly distinguishable from actual values",
            "dependencies": [],
            "details": "Modify placeholder text in SendMailDialog.tsx:235, 250, 261 to be more obviously placeholders. Consider using lighter gray color, italic styling, or different placeholder text that's clearly not a real email address. Update CSS classes if needed to make placeholders more visually distinct from user input.",
            "status": "done",
            "testStrategy": "Verify placeholder text is clearly distinguishable from actual input values and doesn't contribute to the perception that fields are pre-filled",
            "parentId": "undefined"
          }
        ],
        "complexity": 8,
        "recommendedSubtasks": 12,
        "expansionPrompt": "Implement full testing strategy: 1) Create unit tests for all IMAP operations using mock servers, 2) Set up mock IMAP server infrastructure, 3) Implement integration tests using real IMAP adapters for Gmail, 4) Add integration tests for Outlook servers, 5) Add integration tests for standard IMAP servers, 6) Create end-to-end tests covering complete user workflows through REST interface, 7) Create end-to-end tests for MCP interface workflows, 8) Set up performance benchmarks for concurrent connection handling, 9) Achieve >90% code coverage across all modules, 10) Test all error scenarios and edge cases, 11) Implement performance tests meeting requirements (<500ms folder list, <200ms email fetch, 100+ concurrent connections), 12) Set up continuous testing infrastructure"
      },
      {
        "id": "17",
        "title": "Fix email body rendering issues - HTML/image artifacts showing as raw text instead of being properly rendered",
        "description": "Fix the frontend EmailBody component to properly render HTML content and display images/links correctly instead of showing raw text",
        "details": "The current implementation in EmailBody.tsx:303 only displays the plain text body (email.body_text) using whitespace-pre-wrap styling, which causes HTML content and embedded images to appear as raw text/artifacts. The fix involves: 1) Check if email.html_body is available from the backend (already stored in cache.rs and available via the REST API), 2) Modify the EmailBody component to conditionally render HTML content using dangerouslySetInnerHTML when HTML is available, with proper sanitization, 3) Add CSS styles to handle image display, link styling, and proper HTML formatting, 4) Implement a toggle between HTML and plain text views for user preference, 5) Add security measures to sanitize HTML content before rendering to prevent XSS attacks, 6) Update the email fetching logic to include html_body field in the API response. The backend already stores both text_body and html_body in the database (migrations/001_create_schema.sql:101-102) and the IMAP parsing extracts both via mail_parser (imap/types.rs:737-738).",
        "testStrategy": "Test by: 1) Sending HTML emails with embedded images and links to test accounts, 2) Verify HTML content renders properly with images displayed and links clickable, 3) Test the plain text fallback when no HTML is available, 4) Verify HTML/plain text toggle functionality works, 5) Test with malicious HTML content to ensure sanitization prevents XSS, 6) Test responsive display on different screen sizes, 7) Verify that emails without HTML content still display plain text correctly, 8) Test performance with large HTML emails containing multiple images",
        "status": "done",
        "dependencies": [
          "4"
        ],
        "priority": "medium",
        "subtasks": [],
        "complexity": 5,
        "recommendedSubtasks": 6,
        "expansionPrompt": "Set up automated development pipeline: 1) Configure GitHub Actions or similar CI/CD system, 2) Set up automated testing on multiple Rust versions and platforms, 3) Add security scanning and dependency vulnerability checks, 4) Configure automated releases with proper semantic versioning, 5) Add Docker image building and container registry publishing, 6) Test CI/CD pipeline with pull requests and verify deployment automation"
      },
      {
        "id": "18",
        "title": "Add 'Show Images' button to email viewer for privacy protection",
        "description": "Implement a privacy-focused image loading control in the email viewer with an optional button similar to Thunderbird, where images are blocked by default to prevent tracking.",
        "details": "Based on the current EmailBody.tsx implementation that only displays plain text (line 303 uses whitespace-pre-wrap on email.body_text), add image privacy controls when displaying HTML emails: 1) Add a state variable `showImages` (default false) to control image display, 2) When rendering HTML content using email.html_body (which is already available from the backend as seen in cache.rs), implement a two-stage rendering approach: first render HTML with all img src attributes stripped/blocked, 3) Add a 'Show Images' button (using existing Button component and Eye/EyeOff icons from lucide-react) that appears when HTML content contains images, 4) When clicked, re-render the HTML with images enabled, 5) Use DOMParser to safely detect and modify img tags before dangerouslySetInnerHTML rendering, 6) Add user preference persistence via localStorage to remember the choice per sender/domain, 7) Style the button consistently with existing Reply/Forward buttons in the header area (lines 262-284), 8) Ensure the feature works with the existing HTML/text toggle functionality mentioned in Task 17",
        "testStrategy": "Test by: 1) Sending HTML emails with embedded images and tracking pixels to test accounts, 2) Verify images are blocked by default and 'Show Images' button appears, 3) Test button functionality enables images properly, 4) Test localStorage persistence remembers preference, 5) Verify no external requests are made when images are blocked (check network tab), 6) Test with various email clients (Gmail, Outlook, etc.) to ensure compatibility, 7) Test the feature works alongside Task 17's HTML rendering improvements, 8) Verify button styling matches existing UI components",
        "status": "done",
        "dependencies": [
          "17"
        ],
        "priority": "medium",
        "subtasks": [],
        "complexity": 3,
        "recommendedSubtasks": 4,
        "expansionPrompt": "Complete deployment resources: 1) Create deployment guides for standalone binary, Docker, and Kubernetes, 2) Document all configuration options and environment variables, 3) Create deployment scripts and Docker Compose files, 4) Add monitoring, logging configuration examples, and security best practices documentation"
      },
      {
        "id": "19",
        "title": "Add tabs to MCP Email Tools widget in web UI - one tab for low-level MCP tools, another tab for high-level AI-powered MCP tools",
        "description": "Enhance the existing McpTools component to display two separate tabs: one showing all the existing low-level MCP tools and another showing the high-level AI-powered MCP tools",
        "details": "Modify the existing McpTools.tsx component (src/dashboard/components/McpTools.tsx) to use Radix UI Tabs component from components/ui/tabs.tsx. The component should: 1) Import and use Tabs, TabsList, TabsTrigger, and TabsContent from '../ui/tabs', 2) Create two tab triggers: 'Low-Level Tools' and 'AI Tools', 3) Move existing tool fetching and display logic into the 'Low-Level Tools' tab content, 4) Add a new API endpoint fetch to get high-level tools from the backend endpoint '/dashboard/mcp/high-level-tools' (which needs to be implemented to call get_mcp_high_level_tools_jsonrpc_format() from high_level_tools.rs), 5) Display the high-level tools in the 'AI Tools' tab with the same UI pattern as existing tools, 6) Maintain all existing functionality including parameter auto-filling, execution, and result display for both tool types, 7) Update the header to show total tools from both tabs, 8) Ensure proper state management so expanding/collapsing tools, parameters, and results work independently between tabs. The backend route handler should call execute_high_level_tool() for AI tool executions and existing execute_mcp_tool_inner() for low-level tools.",
        "testStrategy": "Test by: 1) Verifying both tabs are visible and clickable in the MCP Tools widget, 2) Confirming the 'Low-Level Tools' tab shows existing tools with unchanged functionality, 3) Verifying the 'AI Tools' tab displays the 12 high-level tools (process_email_instructions, draft_reply, draft_email, list_accounts, etc.), 4) Testing parameter auto-filling works in both tabs based on current email context, 5) Testing tool execution works correctly for both low-level and high-level tools with proper API routing, 6) Verifying results display properly in both tabs, 7) Testing tab switching preserves expanded tool states and parameter values, 8) Confirming the total tool count in header updates correctly when switching tabs",
        "status": "done",
        "dependencies": [
          "3",
          "5"
        ],
        "priority": "medium",
        "subtasks": []
      },
      {
        "id": "20",
        "title": "Add MCP server enable/disable controls to Email Assistant chatbot widget",
        "description": "Implement checkboxes/dropdown controls in ChatbotPanel to enable/disable individual MCP servers (low-level and high-level) for the AI assistant to use during conversations.",
        "details": "Based on the current ChatbotPanel.tsx (lines 334-677) and McpTools.tsx components, add MCP server configuration controls to the chatbot: 1) Add a settings dropdown menu next to the debug toggle button in the ChatbotPanel header (around line 364), 2) Create a new state for tracking enabled/disabled MCP servers with localStorage persistence similar to debugMode (line 72-74), 3) Add a collapsible settings panel that shows two sections: 'Low-Level Tools' and 'High-Level AI Tools', 4) For low-level tools, fetch from existing '/dashboard/mcp/tools' endpoint (line 98 in McpTools.tsx), 5) For high-level tools, create new endpoint '/dashboard/mcp/high-level-tools' that calls get_mcp_high_level_tools_jsonrpc_format() from high_level_tools.rs:11, 6) Display each tool as a checkbox with the tool name and description, allowing users to individually enable/disable tools, 7) Pass the enabled tools list to the chatbot query (in the ChatbotQuery interface) so the backend can filter available tools during AI conversations, 8) Use consistent UI patterns from the existing codebase: Radix UI components (checkbox.tsx, collapsible.tsx), similar styling to the debug panel (lines 549-644), and localStorage persistence pattern.",
        "testStrategy": "Test by: 1) Verifying the settings dropdown appears in the chatbot header and is functional, 2) Confirming both low-level and high-level tools are fetched and displayed correctly with checkboxes, 3) Testing that individual tool enable/disable states persist across browser sessions via localStorage, 4) Verifying the enabled tools list is correctly passed to chatbot queries and affects AI responses, 5) Testing the UI responsiveness and proper styling consistency with existing components, 6) Ensuring the new high-level tools endpoint returns the expected tool definitions from the backend.",
        "status": "done",
        "dependencies": [
          "19"
        ],
        "priority": "medium",
        "subtasks": []
      },
      {
        "id": "21",
        "title": "Disable thinking mode in Qwen3 model by setting enable_thinking=False in Ollama provider configuration",
        "description": "Modify Ollama provider to support additional_config parameters and update Qwen3 model configuration to disable thinking mode for faster response times.",
        "details": "Update the Ollama provider implementation to read and apply additional_config parameters from the ai_model_configurations table: 1) Modify OllamaChatRequest struct in src/dashboard/services/ai/provider/ollama.rs to include optional additional parameters field, 2) Update OllamaAdapter::generate_response() method to fetch model configuration using get_model_config() and parse additional_config JSON to extract provider-specific parameters, 3) Add logic to merge additional_config parameters into the Ollama API request payload, 4) Use set_model_config() to update the Qwen3 model configuration with additional_config JSON: {\"enable_thinking\": false}, 5) Test that the parameter is properly passed to Ollama API and that thinking blocks are no longer generated in responses. The additional_config field should be parsed as JSON and merged into the request body sent to Ollama's /v1/chat/completions endpoint.",
        "testStrategy": "Test by: 1) Verifying that the Ollama provider properly reads additional_config from database and parses JSON parameters, 2) Confirming that enable_thinking=false parameter is included in API requests to Ollama for Qwen3 model, 3) Testing that Qwen3 responses no longer contain <think> blocks and show improved response speed, 4) Verifying that other models without this configuration continue to work normally, 5) Testing configuration updates through MCP tools to ensure the additional_config field can be modified and persisted correctly.",
        "status": "done",
        "dependencies": [
          "2",
          "4"
        ],
        "priority": "medium",
        "subtasks": [],
        "updatedAt": "2026-01-24T14:18:16.982Z"
      },
      {
        "id": "22",
        "title": "Fix permissive CORS configuration in main.rs",
        "description": "Replace the permissive CORS configuration that allows any origin, method, and header with a secure whitelist-based approach using environment variables to prevent CSRF attacks.",
        "details": "Update the CORS configuration in src/main.rs (lines 270-277) to implement a secure origin whitelist:\n\n1) Add a new environment variable ALLOWED_ORIGINS that accepts a comma-separated list of allowed origins (e.g., \"http://localhost:3000,https://dashboard.example.com\")\n\n2) Replace the current permissive configuration:\n   ```rust\n   Cors::default()\n       .allow_any_origin()\n       .allow_any_method()\n       .allow_any_header()\n   ```\n\n3) With a secure configuration:\n   ```rust\n   let allowed_origins = std::env::var(\"ALLOWED_ORIGINS\")\n       .unwrap_or_else(|_| \"http://localhost:3000\".to_string())\n       .split(',')\n       .map(|s| s.trim().to_string())\n       .collect::<Vec<String>>();\n   \n   let cors = Cors::default()\n       .allowed_origins(\n           allowed_origins\n               .iter()\n               .map(|origin| origin.parse::<HeaderValue>().unwrap())\n               .collect::<Vec<_>>()\n               .as_slice()\n       )\n       .allowed_methods(vec![\"GET\", \"POST\", \"PUT\", \"DELETE\", \"OPTIONS\"])\n       .allowed_headers(vec![\n           header::CONTENT_TYPE,\n           header::AUTHORIZATION,\n           header::ACCEPT,\n       ])\n       .supports_credentials()\n       .max_age(3600);\n   ```\n\n4) Update the .env.example file to include the new ALLOWED_ORIGINS variable with sensible defaults\n\n5) Add validation to ensure at least one origin is configured and that origins are valid URLs\n\n6) Consider adding a warning log if ALLOWED_ORIGINS is not set, defaulting to localhost only for development safety\n\n7) Ensure the CORS middleware properly handles preflight OPTIONS requests\n\n8) Update any deployment documentation to specify the ALLOWED_ORIGINS configuration requirement",
        "testStrategy": "Verify the CORS fix by:\n\n1) Start the server without ALLOWED_ORIGINS set and confirm it defaults to localhost:3000 only\n2) Set ALLOWED_ORIGINS=\"http://localhost:3000,http://localhost:5173\" and restart the server\n3) Test that requests from allowed origins work correctly:\n   - Make API calls from http://localhost:3000 and verify they succeed\n   - Make API calls from http://localhost:5173 and verify they succeed\n4) Test that requests from non-allowed origins are blocked:\n   - Use curl or a browser from http://localhost:8080 and verify CORS error\n   - Try making requests from https://evil.com and confirm they're rejected\n5) Verify preflight OPTIONS requests are handled correctly for allowed origins\n6) Test with credentials (cookies/auth headers) to ensure supports_credentials() works\n7) Check server logs for appropriate warnings when ALLOWED_ORIGINS is not configured\n8) Verify that malformed origins in ALLOWED_ORIGINS cause server startup to fail with clear error message",
        "status": "done",
        "dependencies": [
          "32"
        ],
        "priority": "high",
        "subtasks": [],
        "updatedAt": "2026-01-23T10:44:12.311Z"
      },
      {
        "id": "23",
        "title": "Fix origin validation bypass in MCP HTTP backend",
        "description": "Fix critical security vulnerability in src/api/mcp_http.rs (lines 171-189) where origin validation accepts any domain containing 'localhost' and allows requests with no Origin header, enabling CSRF attacks.",
        "details": "Update the origin validation logic in src/api/mcp_http.rs to implement secure origin checking:\n\n1) **Fix substring matching vulnerability** (line ~175-180):\n   - Replace the current logic that accepts any origin containing 'localhost' (e.g., evil.localhost.com)\n   - Implement exact string matching for allowed origins\n   - Use a whitelist approach with full origin strings including protocol and port\n\n2) **Require Origin header on all requests**:\n   - Remove the logic that allows requests with missing Origin headers\n   - Return 403 Forbidden for requests without Origin header\n   - Add proper error message: \"Origin header required\"\n\n3) **Implement secure origin validation**:\n   ```rust\n   // Add at top of file\n   use std::env;\n   \n   // In the origin validation section\n   let allowed_origins = env::var(\"ALLOWED_MCP_ORIGINS\")\n       .unwrap_or_else(|_| \"http://localhost:3000\".to_string())\n       .split(',')\n       .map(|s| s.trim().to_string())\n       .collect::<Vec<String>>();\n   \n   // Validate origin\n   if let Some(origin) = req.headers().get(\"Origin\") {\n       let origin_str = origin.to_str().unwrap_or(\"\");\n       if !allowed_origins.contains(&origin_str.to_string()) {\n           return Ok(Response::builder()\n               .status(StatusCode::FORBIDDEN)\n               .body(Body::from(\"Origin not allowed\"))\n               .unwrap());\n       }\n   } else {\n       return Ok(Response::builder()\n           .status(StatusCode::FORBIDDEN)\n           .body(Body::from(\"Origin header required\"))\n           .unwrap());\n   }\n   ```\n\n4) **Add environment variable configuration**:\n   - Support ALLOWED_MCP_ORIGINS environment variable\n   - Accept comma-separated list of full origins (e.g., \"http://localhost:3000,http://localhost:5173,https://app.example.com\")\n   - Default to \"http://localhost:3000\" if not set\n\n5) **Update CORS headers in response**:\n   - Set Access-Control-Allow-Origin to the specific requesting origin (not \"*\")\n   - Only set it if the origin is in the allowed list\n\n6) **Consider preflight requests**:\n   - Ensure OPTIONS requests also validate origins\n   - Return appropriate CORS headers only for allowed origins",
        "testStrategy": "Verify the security fix with comprehensive testing:\n\n1) **Test substring matching fix**:\n   - Send request with Origin: http://evil.localhost.com - should be rejected (403)\n   - Send request with Origin: http://localhost.attacker.com - should be rejected (403)\n   - Send request with Origin: http://localhost:3000 - should be allowed (200)\n\n2) **Test missing Origin header enforcement**:\n   - Use curl without Origin header: `curl http://localhost:8080/mcp/tools/list` - should be rejected (403)\n   - Use curl with valid Origin: `curl -H \"Origin: http://localhost:3000\" http://localhost:8080/mcp/tools/list` - should work\n\n3) **Test environment variable configuration**:\n   - Set ALLOWED_MCP_ORIGINS=\"http://localhost:3000,http://localhost:5173\"\n   - Verify requests from localhost:3000 work\n   - Verify requests from localhost:5173 work\n   - Verify requests from localhost:8080 are rejected\n\n4) **Test exact matching with ports**:\n   - Origin: http://localhost:3000 with ALLOWED_MCP_ORIGINS=\"http://localhost:3001\" - should fail\n   - Origin: http://localhost:3001 with ALLOWED_MCP_ORIGINS=\"http://localhost:3001\" - should work\n\n5) **Test CORS response headers**:\n   - Verify Access-Control-Allow-Origin is set to the specific origin (not \"*\")\n   - Verify it's only set when origin is allowed\n\n6) **Test preflight OPTIONS requests**:\n   - Send OPTIONS request with valid origin - should return proper CORS headers\n   - Send OPTIONS request with invalid origin - should be rejected",
        "status": "done",
        "dependencies": [
          "22"
        ],
        "priority": "high",
        "subtasks": [],
        "updatedAt": "2026-01-23T11:04:42.187Z"
      },
      {
        "id": "24",
        "title": "Remove hardcoded test credentials and require configured API keys",
        "description": "Remove all hardcoded test API keys and credentials from the codebase, require explicit configuration of all API keys, add key expiration support, and update documentation for secure key generation.",
        "details": "Fix critical security vulnerabilities in API key management by removing all hardcoded credentials and test keys:\n\n1) **Remove hardcoded test key initialization in src/api/auth.rs**:\n   - Delete or comment out the ApiKeyStore::init_with_defaults() method that seeds a test key with Admin scope at startup\n   - Ensure no API keys are automatically created when the application starts\n   - Modify the initialization logic to require explicit key configuration through environment variables or secure configuration files\n\n2) **Remove test credentials from .env.example**:\n   - Remove the line `RUSTYMAIL_API_KEY=test-rustymail-key-2024` from .env.example\n   - Replace with a placeholder like `RUSTYMAIL_API_KEY=your-secure-api-key-here`\n   - Add comments explaining that users must generate their own secure API keys\n\n3) **Implement API key expiration support**:\n   - Add an `expires_at` field to the API key storage structure (likely in ApiKeyStore)\n   - Modify the key validation logic to check expiration timestamps\n   - Return 401 Unauthorized for expired keys with appropriate error messages\n   - Consider adding a configurable default expiration period (e.g., 90 days)\n\n4) **Remove any hardcoded IMAP credentials**:\n   - Search the codebase for any hardcoded IMAP usernames, passwords, or server configurations\n   - Ensure all IMAP credentials must be provided through secure configuration\n   - Update any test configurations to use environment variables instead\n\n5) **Add secure key generation documentation**:\n   - Create a new section in the README or a separate SECURITY.md file\n   - Document how to generate cryptographically secure API keys (e.g., using openssl rand -hex 32)\n   - Explain the importance of key rotation and expiration\n   - Provide examples of secure key storage practices\n   - Document the required scopes and permissions for different API operations\n\n6) **Update application startup logic**:\n   - Add validation to ensure required API keys are configured before the application starts\n   - Provide clear error messages if required keys are missing\n   - Consider implementing a setup wizard or initialization script for first-time configuration",
        "testStrategy": "Verify the security improvements with comprehensive testing:\n\n1) **Test removal of hardcoded keys**:\n   - Start the application with a clean environment (no API keys configured)\n   - Verify the application refuses to start or enters a safe mode without any pre-configured keys\n   - Confirm no test keys are accessible through the API\n\n2) **Test API key expiration**:\n   - Create an API key with a short expiration time (e.g., 1 minute in the future)\n   - Make successful API calls with the key\n   - Wait for the key to expire\n   - Verify subsequent API calls return 401 Unauthorized with an \"expired key\" error message\n\n3) **Test IMAP credential requirements**:\n   - Attempt to use IMAP functionality without configuring credentials\n   - Verify appropriate error messages are returned\n   - Configure valid IMAP credentials through environment variables\n   - Confirm IMAP functionality works correctly with configured credentials\n\n4) **Test .env.example changes**:\n   - Copy .env.example to .env\n   - Verify the application doesn't start with placeholder values\n   - Replace placeholders with valid keys and confirm successful startup\n\n5) **Security audit**:\n   - Search the entire codebase for strings like \"test\", \"default\", \"admin\" in authentication contexts\n   - Verify no hardcoded credentials remain in any source files\n   - Check that all authentication-related configuration comes from environment variables or secure config files\n\n6) **Documentation verification**:\n   - Follow the new secure key generation documentation to create API keys\n   - Verify the generated keys work correctly with the application\n   - Confirm all security best practices are clearly explained",
        "status": "done",
        "dependencies": [
          "22",
          "23"
        ],
        "priority": "high",
        "subtasks": [],
        "updatedAt": "2026-01-23T11:07:28.410Z"
      },
      {
        "id": "25",
        "title": "Add API-key and scope validation to all MCP endpoints",
        "description": "Implement mandatory API-key validation middleware for all MCP routes in mcp_http.rs with per-tool scope requirements and proper error responses for authentication/authorization failures.",
        "details": "Modify src/api/mcp_http.rs to add comprehensive security to mcp_post_handler and mcp_get_handler which currently rely only on weak origin checks:\n\n1) Create a new middleware module src/api/auth/api_key_middleware.rs with:\n   - ApiKey struct containing key, scopes, and metadata\n   - validate_api_key() function that checks against a database table or config file\n   - extract_api_key_from_request() to get key from Authorization header (Bearer token) or X-API-Key header\n   - ApiKeyMiddleware that intercepts all MCP requests before handlers\n\n2) Define scope requirements for each MCP tool:\n   - Low-level tools: email:read, email:write, folder:read, etc.\n   - High-level tools: ai:execute, model:configure, email:draft\n   - Create a tool_scopes mapping in high_level_tools.rs and regular tools module\n\n3) Update mcp_post_handler and mcp_get_handler:\n   - Remove or supplement weak origin check with API key validation\n   - Extract requested tool from the JSON-RPC request\n   - Look up required scopes for the tool\n   - Validate API key has all required scopes\n   - Pass validated API key context to downstream handlers\n\n4) Implement proper error responses:\n   - 401 Unauthorized for missing or invalid API keys\n   - 403 Forbidden for valid key but insufficient scopes\n   - Include WWW-Authenticate header with realm=\"MCP API\"\n   - Return JSON-RPC error format with descriptive messages\n\n5) Create database schema for API keys:\n   ```sql\n   CREATE TABLE api_keys (\n     id SERIAL PRIMARY KEY,\n     key_hash VARCHAR(255) UNIQUE NOT NULL,\n     name VARCHAR(255),\n     scopes TEXT[], -- Array of scope strings\n     created_at TIMESTAMP DEFAULT NOW(),\n     last_used_at TIMESTAMP,\n     is_active BOOLEAN DEFAULT true\n   );\n   ```\n\n6) Add configuration for API key validation:\n   - Environment variable to enable/disable in development\n   - Option to load keys from config file for testing\n   - Rate limiting per API key to prevent abuse",
        "testStrategy": "Test the API key validation thoroughly:\n\n1) Unit tests for api_key_middleware.rs:\n   - Test API key extraction from different header formats\n   - Test scope validation logic with various scope combinations\n   - Test database queries for API key lookup\n\n2) Integration tests for MCP endpoints:\n   - Test requests without API key return 401\n   - Test requests with invalid API key return 401\n   - Test requests with valid key but missing scopes return 403\n   - Test successful requests with proper API key and scopes\n   - Test both mcp_post_handler and mcp_get_handler paths\n\n3) Test each tool's scope requirements:\n   - Verify low-level tools require appropriate read/write scopes\n   - Verify high-level AI tools require elevated scopes\n   - Test scope inheritance (e.g., email:write includes email:read)\n\n4) Security testing:\n   - Attempt to bypass with malformed headers\n   - Test SQL injection in API key lookup\n   - Verify timing attacks don't reveal key existence\n   - Test rate limiting prevents brute force\n\n5) End-to-end testing:\n   - Create test API keys with different scope sets\n   - Verify frontend MCP client can authenticate properly\n   - Test error handling in UI when authentication fails\n   - Verify performance impact is minimal",
        "status": "done",
        "dependencies": [
          "22",
          "23",
          "24"
        ],
        "priority": "high",
        "subtasks": [],
        "updatedAt": "2026-01-23T11:10:41.611Z"
      },
      {
        "id": "26",
        "title": "Implement encryption for stored credentials",
        "description": "Add encryption at rest for all sensitive credentials including IMAP/SMTP passwords in database and JSON config files, with support for application-level encryption or external KMS integration.",
        "details": "Implement a comprehensive encryption solution for all stored credentials:\n\n1) Create encryption module at src/dashboard/services/security/encryption.rs:\n   - Define CredentialEncryption trait with encrypt() and decrypt() methods\n   - Implement ApplicationLevelEncryption using AES-256-GCM with master key from ENCRYPTION_MASTER_KEY env var\n   - Implement KmsEncryption for AWS KMS/Azure Key Vault integration (configurable via ENCRYPTION_PROVIDER env var)\n   - Add EncryptionService that selects provider based on configuration\n\n2) Update database schema with migration 005_add_credential_encryption.sql:\n   ```sql\n   ALTER TABLE email_accounts \n   ADD COLUMN password_encrypted BYTEA,\n   ADD COLUMN encryption_metadata JSONB;\n   \n   ALTER TABLE ai_model_configurations\n   ADD COLUMN api_key_encrypted BYTEA,\n   ADD COLUMN encryption_metadata JSONB;\n   ```\n\n3) Modify src/dashboard/models/email_account.rs:\n   - Add password_encrypted and encryption_metadata fields\n   - Update create() and update() methods to encrypt password before storage\n   - Modify get_password() to decrypt on retrieval\n   - Keep backward compatibility during migration\n\n4) Update src/dashboard/models/ai_model_configuration.rs similarly for api_key field\n\n5) Create migration script src/dashboard/services/security/migrate_credentials.rs:\n   - Scan all email_accounts and ai_model_configurations records\n   - For each plaintext credential, encrypt and store in new columns\n   - Verify decryption works correctly\n   - Once verified, null out plaintext columns\n\n6) Update JSON config handling in src/config/mod.rs:\n   - Detect plaintext credentials in config files\n   - Encrypt and rewrite config with encrypted values\n   - Add encryption_metadata to track encryption method\n\n7) Add key rotation support:\n   - Implement rotate_encryption_key() method\n   - Re-encrypt all credentials with new key\n   - Update encryption_metadata with rotation timestamp",
        "testStrategy": "Verify encryption implementation with comprehensive testing:\n\n1) Unit tests for encryption module:\n   - Test AES-256-GCM encryption/decryption with known test vectors\n   - Verify different length passwords encrypt correctly\n   - Test error handling for invalid keys or corrupted data\n   - Mock KMS integration tests\n\n2) Integration tests for database operations:\n   - Create email account with password, verify it's stored encrypted\n   - Retrieve account and confirm password decrypts correctly\n   - Test migration script on test data with mix of plaintext/encrypted records\n   - Verify AI model API keys are encrypted similarly\n\n3) End-to-end testing:\n   - Set ENCRYPTION_MASTER_KEY and restart application\n   - Create new email account via API/UI\n   - Query database directly to confirm password_encrypted is populated and password is null\n   - Use account for IMAP/SMTP operations to verify decryption works\n   - Test with missing ENCRYPTION_MASTER_KEY to ensure proper error handling\n\n4) Security validation:\n   - Verify encrypted values are different even for same plaintext (due to random IV)\n   - Confirm encryption metadata includes algorithm version for future compatibility\n   - Test key rotation functionality with multiple credentials",
        "status": "done",
        "dependencies": [
          "24"
        ],
        "priority": "medium",
        "subtasks": [],
        "updatedAt": "2026-01-23T11:54:41.880Z"
      },
      {
        "id": "27",
        "title": "Fix path traversal vulnerability in attachment_storage.rs",
        "description": "Implement secure path canonicalization and containment checks in attachment_storage.rs to prevent directory traversal attacks via malicious file paths or symlinks.",
        "details": "Fix the path traversal vulnerability in attachment_storage.rs by implementing comprehensive path security measures:\n\n1) **Add path canonicalization**:\n   - Import std::fs::canonicalize() to resolve all symbolic links and relative path components\n   - Before any file operation, canonicalize both the requested path and the storage root directory\n   - Handle canonicalization errors gracefully (non-existent paths, permission issues)\n\n2) **Implement strict containment validation**:\n   - Create a validate_path_containment() function that:\n     - Canonicalizes the requested file path\n     - Canonicalizes the attachments storage root directory\n     - Uses path.starts_with() to ensure the resolved path is within the storage root\n     - Returns Result<PathBuf, SecurityError> with the safe canonicalized path or error\n   \n3) **Update all file operations**:\n   - Modify save_attachment(), get_attachment(), delete_attachment() to use validate_path_containment()\n   - Replace current basic path component checks with the new validation\n   - Ensure all Path/PathBuf constructions go through validation before use\n\n4) **Handle edge cases**:\n   - Reject null bytes in filenames\n   - Validate against Windows reserved names (CON, PRN, AUX, etc.) if cross-platform\n   - Handle Unicode normalization attacks (different representations of same character)\n   - Prevent TOCTOU attacks by using the validated canonical path for operations\n\n5) **Example implementation**:\n   ```rust\n   use std::path::{Path, PathBuf};\n   use std::fs;\n   \n   #[derive(Debug, thiserror::Error)]\n   enum PathSecurityError {\n       #[error(\"Path traversal attempt detected\")]\n       PathTraversal,\n       #[error(\"Invalid path: {0}\")]\n       InvalidPath(String),\n       #[error(\"Canonicalization failed: {0}\")]\n       CanonicalizationError(#[from] std::io::Error),\n   }\n   \n   fn validate_path_containment(\n       storage_root: &Path,\n       requested_path: &Path\n   ) -> Result<PathBuf, PathSecurityError> {\n       // Canonicalize the storage root\n       let canonical_root = fs::canonicalize(storage_root)?;\n       \n       // Construct full path and canonicalize\n       let full_path = storage_root.join(requested_path);\n       let canonical_path = fs::canonicalize(&full_path)\n           .or_else(|_| {\n               // If file doesn't exist, canonicalize parent and append filename\n               let parent = full_path.parent()\n                   .ok_or_else(|| PathSecurityError::InvalidPath(\"No parent directory\".into()))?;\n               let filename = full_path.file_name()\n                   .ok_or_else(|| PathSecurityError::InvalidPath(\"No filename\".into()))?;\n               \n               let canonical_parent = fs::canonicalize(parent)?;\n               Ok(canonical_parent.join(filename))\n           })?;\n       \n       // Verify the canonical path is within the storage root\n       if !canonical_path.starts_with(&canonical_root) {\n           return Err(PathSecurityError::PathTraversal);\n       }\n       \n       Ok(canonical_path)\n   }\n   ```\n\n6) **Add security logging**:\n   - Log all path traversal attempts with source IP/user info\n   - Include the malicious path in logs for security monitoring\n   - Consider rate limiting after multiple traversal attempts",
        "testStrategy": "Verify the path traversal fix with comprehensive security testing:\n\n1) **Unit tests for path validation**:\n   - Test basic traversal attempts: \"../../../etc/passwd\", \"..\\\\..\\\\windows\\\\system32\"\n   - Test encoded traversals: \"%2e%2e%2f\", \"..%252f\", \"%c0%ae%c0%ae/\"\n   - Test symlink traversal: create symlink pointing outside storage, verify rejection\n   - Test absolute paths: \"/etc/passwd\", \"C:\\\\Windows\\\\System32\"\n   - Test null bytes: \"file.txt\\x00.pdf\"\n   - Test Unicode tricks: \"le.txt\" (ligature), different normalization forms\n\n2) **Integration tests**:\n   - Create test storage directory with known structure\n   - Attempt to save files with malicious paths, verify all are rejected\n   - Test legitimate nested paths work correctly: \"user123/2024/invoice.pdf\"\n   - Verify error messages don't leak system paths\n\n3) **Edge case testing**:\n   - Test very long paths (near filesystem limits)\n   - Test special filenames: \".\", \"..\", \"~\", \"$file\"\n   - Test Windows reserved names: \"CON\", \"PRN\", \"AUX\", \"NUL\"\n   - Test case sensitivity issues on case-insensitive filesystems\n\n4) **TOCTOU race condition test**:\n   - Create a legitimate file\n   - In parallel thread, try to replace it with symlink during validation\n   - Verify the operation uses the validated canonical path\n\n5) **Performance testing**:\n   - Measure overhead of canonicalization on typical operations\n   - Test with deeply nested directory structures\n   - Ensure no significant performance regression\n\n6) **Security audit checklist**:\n   - Verify all file operations use validate_path_containment()\n   - Check no direct Path construction from user input\n   - Confirm error messages don't reveal system structure\n   - Review logs for attempted traversals during testing",
        "status": "done",
        "dependencies": [
          "32"
        ],
        "priority": "medium",
        "subtasks": [],
        "updatedAt": "2026-01-23T11:20:23.188Z"
      },
      {
        "id": "28",
        "title": "Wire rate limiting into REST and MCP API paths",
        "description": "Integrate the existing rate limiting validation from validation.rs into all REST API endpoints and MCP handlers, implementing per-IP and per-API-key limits with proper 429 responses and rate limit headers.",
        "details": "Implement comprehensive rate limiting across all API surfaces:\n\n1) **Create rate limiting middleware for REST APIs**:\n   - Create src/dashboard/middleware/rate_limit.rs\n   - Import existing rate limiting logic from validation.rs\n   - Implement RateLimitMiddleware that extracts client IP and API key from requests\n   - Use a token bucket or sliding window algorithm for tracking request counts\n   - Store rate limit state in memory with Arc<Mutex<HashMap>> or use Redis for distributed deployments\n\n2) **Configure rate limits via environment variables**:\n   - Add RATE_LIMIT_PER_MINUTE (default: 60)\n   - Add RATE_LIMIT_PER_HOUR (default: 1000)\n   - Add RATE_LIMIT_PER_IP_MINUTE (default: 30)\n   - Add RATE_LIMIT_PER_IP_HOUR (default: 500)\n   - Support different limits for authenticated (API key) vs anonymous requests\n\n3) **Integrate middleware into REST API routes**:\n   - In main.rs, wrap all API routes with rate limiting middleware\n   - Apply before authentication middleware to protect against auth bypass attempts\n   - Example:\n   ```rust\n   .wrap(RateLimitMiddleware::new(rate_limit_config))\n   .wrap(cors)\n   .wrap(Logger::default())\n   ```\n\n4) **Add rate limiting to MCP handlers**:\n   - In each MCP handler function, add rate limit check at the beginning\n   - Extract client identifier from MCP context (connection ID or client metadata)\n   - Use the same rate limiting logic but with MCP-specific limits\n   - Return appropriate MCP error response when rate limited\n\n5) **Implement 429 Too Many Requests responses**:\n   - For REST APIs: Return HTTP 429 status with JSON error body\n   - Include retry-after header indicating when client can retry\n   - Error response format:\n   ```json\n   {\n     \"error\": \"rate_limit_exceeded\",\n     \"message\": \"Too many requests. Please retry after 60 seconds.\",\n     \"retry_after\": 60\n   }\n   ```\n\n6) **Add rate limit headers to all responses**:\n   - X-RateLimit-Limit: Maximum requests allowed\n   - X-RateLimit-Remaining: Requests remaining in current window\n   - X-RateLimit-Reset: Unix timestamp when the rate limit resets\n   - Add these headers even for successful requests\n\n7) **Handle edge cases**:\n   - Properly extract real client IP behind proxies (X-Forwarded-For, X-Real-IP)\n   - Implement IP whitelist for internal services (via RATE_LIMIT_WHITELIST_IPS env var)\n   - Graceful degradation if rate limit storage fails\n   - Different rate limits for different API endpoints (e.g., higher for read, lower for write)",
        "testStrategy": "Verify rate limiting implementation with comprehensive testing:\n\n1) **Unit tests for rate limiting logic**:\n   - Test token bucket/sliding window algorithm correctness\n   - Verify per-minute and per-hour limits work independently\n   - Test IP-based vs API-key-based rate limiting\n   - Verify rate limit reset timing\n\n2) **Integration tests for REST API**:\n   - Send requests up to the limit and verify all succeed\n   - Send one more request and verify 429 response with correct headers\n   - Wait for rate limit reset and verify requests work again\n   - Test with different IPs and API keys to ensure isolation\n\n3) **MCP handler rate limiting tests**:\n   - Mock MCP requests and verify rate limiting applies\n   - Test that rate limited MCP calls return appropriate error responses\n   - Verify MCP rate limits are independent from REST API limits\n\n4) **Header validation tests**:\n   - Verify all responses include X-RateLimit-* headers\n   - Check header values decrease correctly with each request\n   - Verify Reset header contains valid future timestamp\n\n5) **Load testing**:\n   - Use Apache Bench or similar to send concurrent requests\n   - Verify rate limiting holds under high concurrency\n   - Test with multiple IPs to ensure no cross-contamination\n\n6) **Configuration tests**:\n   - Start server with custom rate limit env vars\n   - Verify limits match configured values\n   - Test with missing env vars to ensure defaults work\n\n7) **Security tests**:\n   - Attempt to bypass with spoofed headers\n   - Verify whitelisted IPs bypass rate limits\n   - Test rate limiting works before authentication",
        "status": "done",
        "dependencies": [
          "22"
        ],
        "priority": "medium",
        "subtasks": [],
        "updatedAt": "2026-01-23T11:26:45.106Z"
      },
      {
        "id": "29",
        "title": "Pin git dependencies to specific commit SHAs",
        "description": "Update Cargo.toml to pin all git dependencies (including rmcp crate) to specific commit SHAs to prevent supply chain attacks, document the pinned versions, and establish a periodic review process for dependency updates.",
        "details": "Implement secure dependency pinning to protect against supply chain attacks by ensuring all git dependencies reference immutable commit SHAs:\n\n1) **Audit current git dependencies in Cargo.toml**:\n   - Search for all dependencies using git = \"...\" syntax\n   - Identify the rmcp crate and any other git-based dependencies\n   - For each dependency, determine the current branch/tag being tracked\n   - Clone each repository and identify the exact commit SHA currently in use\n\n2) **Pin dependencies to specific commit SHAs**:\n   - Replace branch/tag references with rev = \"SHA\" for each git dependency\n   - Example transformation:\n     ```toml\n     # Before (vulnerable to upstream changes):\n     rmcp = { git = \"https://github.com/example/rmcp\", branch = \"main\" }\n     \n     # After (pinned to specific commit):\n     rmcp = { git = \"https://github.com/example/rmcp\", rev = \"a1b2c3d4e5f6...\" }\n     ```\n   - Run `cargo update` to ensure the lock file reflects the pinned versions\n   - Verify the application builds and tests pass with pinned dependencies\n\n3) **Document pinned dependencies**:\n   - Create docs/dependency-pins.md with a table documenting:\n     - Dependency name\n     - Repository URL\n     - Pinned commit SHA\n     - Commit date and author\n     - Version/tag the commit corresponds to (if any)\n     - Brief description of why this specific commit was chosen\n     - Last review date\n   - Add comments in Cargo.toml above each pinned dependency explaining the version\n\n4) **Establish review process**:\n   - Create .github/workflows/dependency-review.yml for monthly automated checks:\n     ```yaml\n     name: Dependency Review\n     on:\n       schedule:\n         - cron: '0 0 1 * *'  # Monthly on the 1st\n       workflow_dispatch:\n     \n     jobs:\n       review-git-deps:\n         runs-on: ubuntu-latest\n         steps:\n           - uses: actions/checkout@v3\n           - name: Check for updates\n             run: |\n               # Script to check each pinned repo for new commits\n               # Create issues for dependencies with updates available\n     ```\n   - Add a SECURITY.md section on dependency update procedures\n   - Document the review checklist:\n     - Check upstream repository for security advisories\n     - Review commit history since pinned version\n     - Test updates in isolated environment\n     - Update both Cargo.toml and docs/dependency-pins.md\n\n5) **Add CI validation**:\n   - Create a GitHub Action that fails if any git dependencies lack rev pins\n   - Add pre-commit hook to warn developers about unpinned dependencies\n   - Include dependency pinning in security audit checklist",
        "testStrategy": "Verify the dependency pinning implementation with comprehensive testing:\n\n1) **Validate all git dependencies are pinned**:\n   - Parse Cargo.toml and verify every git dependency has a `rev` field\n   - Ensure no git dependencies use `branch`, `tag`, or default to HEAD\n   - Run `cargo tree` to confirm resolved dependencies match pinned SHAs\n\n2) **Test build reproducibility**:\n   - Delete Cargo.lock and run `cargo build` on different machines\n   - Verify the exact same dependency versions are resolved\n   - Compare checksums of built artifacts to ensure deterministic builds\n\n3) **Verify documentation completeness**:\n   - Check docs/dependency-pins.md exists and contains all git dependencies\n   - Validate each entry has all required fields (SHA, date, reason, etc.)\n   - Cross-reference Cargo.toml pins with documentation\n\n4) **Test automated review process**:\n   - Manually trigger the dependency review workflow\n   - Verify it correctly identifies outdated dependencies\n   - Confirm it creates GitHub issues with appropriate labels and details\n   - Test the workflow with a intentionally outdated test dependency\n\n5) **Security validation**:\n   - Attempt to modify a git dependency to use a branch reference\n   - Verify CI pipeline fails with clear error message\n   - Test pre-commit hook warns about unpinned dependencies\n   - Simulate a supply chain attack by creating a malicious fork and verify pinning prevents it\n\n6) **Integration testing**:\n   - Run full test suite with pinned dependencies\n   - Deploy to staging environment and verify functionality\n   - Monitor for any performance or compatibility issues\n   - Test rollback procedure if a pinned dependency causes issues",
        "status": "done",
        "dependencies": [
          "32"
        ],
        "priority": "low",
        "subtasks": [],
        "updatedAt": "2026-01-23T11:57:58.341Z"
      },
      {
        "id": "30",
        "title": "Reduce unwrap/expect usage in request handlers",
        "description": "Replace 599 unwrap/expect calls in request handlers with proper Result handling and error responses, focusing on handlers that process external input to prevent panics from unexpected data.",
        "details": "Systematically eliminate panic-inducing unwrap/expect calls from request handlers to improve application stability and security:\n\n1) **Audit and prioritize unwrap/expect usage**:\n   - Run `rg -c \"\\.unwrap\\(\\)|\\.expect\\(\" src/` to get current count and locations\n   - Focus on high-risk areas: src/api/, src/dashboard/handlers/, and MCP handlers\n   - Prioritize handlers that process external input: API endpoints, form submissions, file uploads\n   - Create a tracking spreadsheet with file, line number, risk level, and replacement strategy\n\n2) **Define proper error types**:\n   - Create src/errors/handler_errors.rs with custom error types:\n   ```rust\n   #[derive(Debug, thiserror::Error)]\n   pub enum HandlerError {\n       #[error(\"Invalid input: {0}\")]\n       InvalidInput(String),\n       #[error(\"Database error: {0}\")]\n       Database(#[from] sqlx::Error),\n       #[error(\"Serialization error: {0}\")]\n       Serialization(#[from] serde_json::Error),\n       #[error(\"IO error: {0}\")]\n       Io(#[from] std::io::Error),\n       #[error(\"Authentication failed\")]\n       Unauthorized,\n       #[error(\"Resource not found\")]\n       NotFound,\n   }\n   ```\n   - Implement ResponseError trait for automatic HTTP response conversion\n\n3) **Replace unwrap/expect in API handlers**:\n   - Convert unwrap() to ? operator where possible\n   - Replace expect() with map_err() to provide context:\n   ```rust\n   // Before\n   let user_id = req.param(\"id\").unwrap().parse::<i32>().unwrap();\n   \n   // After\n   let user_id = req.param(\"id\")\n       .ok_or(HandlerError::InvalidInput(\"Missing user ID\".into()))?\n       .parse::<i32>()\n       .map_err(|_| HandlerError::InvalidInput(\"Invalid user ID format\".into()))?;\n   ```\n\n4) **Handle JSON parsing safely**:\n   - Replace serde_json::from_str().unwrap() with proper error handling:\n   ```rust\n   // Before\n   let config: Config = serde_json::from_str(&body).unwrap();\n   \n   // After\n   let config: Config = serde_json::from_str(&body)\n       .map_err(|e| HandlerError::InvalidInput(format!(\"Invalid JSON: {}\", e)))?;\n   ```\n\n5) **Fix database query handling**:\n   - Replace query unwraps with proper Result propagation:\n   ```rust\n   // Before\n   let user = sqlx::query_as!(User, \"SELECT * FROM users WHERE id = $1\", id)\n       .fetch_one(&pool)\n       .await\n       .unwrap();\n   \n   // After\n   let user = sqlx::query_as!(User, \"SELECT * FROM users WHERE id = $1\", id)\n       .fetch_one(&pool)\n       .await\n       .map_err(|e| match e {\n           sqlx::Error::RowNotFound => HandlerError::NotFound,\n           _ => HandlerError::Database(e),\n       })?;\n   ```\n\n6) **Update MCP handlers**:\n   - Focus on mcp_http.rs handlers that process tool calls\n   - Replace unwrap in JSON-RPC parsing and response building\n   - Add proper error responses following JSON-RPC error format\n\n7) **Implement error response middleware**:\n   - Create middleware to convert HandlerError to appropriate HTTP responses\n   - Include error details in development, sanitized messages in production\n   - Add request ID for error tracking",
        "testStrategy": "Verify the unwrap/expect reduction with comprehensive testing:\n\n1) **Static analysis verification**:\n   - Run `rg -c \"\\.unwrap\\(\\)|\\.expect\\(\" src/api/ src/dashboard/handlers/` before and after\n   - Verify significant reduction in count (target: 80%+ reduction in these directories)\n   - Use clippy with `#![warn(clippy::unwrap_used, clippy::expect_used)]` on modified files\n\n2) **Unit tests for error handling**:\n   - Test each HandlerError variant converts to correct HTTP status code\n   - Verify error messages are properly formatted and sanitized\n   - Test error context preservation through map_err chains\n\n3) **Integration tests for API endpoints**:\n   - Send malformed JSON to endpoints, verify 400 Bad Request responses\n   - Test with invalid IDs, verify 404 Not Found responses\n   - Send requests missing required fields, verify descriptive error messages\n   - Test database connection failures return 500 Internal Server Error\n\n4) **Panic testing**:\n   - Set up panic hook to log and alert on any remaining panics\n   - Run fuzzing tests on API endpoints with random/malformed input\n   - Monitor application logs during testing for any panic messages\n\n5) **Load testing for stability**:\n   - Run load tests with mix of valid and invalid requests\n   - Verify no panics occur under high load with bad input\n   - Check error rates remain consistent without crashes",
        "status": "done",
        "dependencies": [
          "25"
        ],
        "priority": "low",
        "subtasks": [],
        "updatedAt": "2026-01-23T12:00:09.734Z"
      },
      {
        "id": "31",
        "title": "Replace unsafe process checks in sync.rs",
        "description": "Replace unsafe blocks used for process state checking in sync.rs with safe Rust alternatives using proper synchronization primitives, documenting any truly necessary unsafe code with safety invariants.",
        "details": "Eliminate unsafe code in sync.rs by implementing safe alternatives for process state checking:\n\n1) **Audit current unsafe usage in sync.rs**:\n   - Identify all unsafe blocks and their purposes (likely checking process states, shared memory access, or FFI calls)\n   - Document what each unsafe block is trying to achieve\n   - Determine if the unsafe code is for performance, FFI, or working around Rust's safety checks\n   - Create a list of safety invariants that the current code assumes\n\n2) **Replace with safe synchronization primitives**:\n   - For shared state access, use Arc<Mutex<T>> or Arc<RwLock<T>> instead of raw pointers\n   - For atomic operations, use std::sync::atomic types (AtomicBool, AtomicUsize, etc.)\n   - For cross-thread communication, use channels (mpsc, crossbeam) instead of shared memory\n   - For process state tracking, consider using a state machine pattern with enums\n\n3) **Implement safe process state management**:\n   ```rust\n   use std::sync::{Arc, RwLock};\n   use std::sync::atomic::{AtomicBool, Ordering};\n   \n   #[derive(Debug, Clone)]\n   enum ProcessState {\n       Idle,\n       Running { pid: u32 },\n       Completed { exit_code: i32 },\n       Failed { error: String },\n   }\n   \n   struct ProcessManager {\n       state: Arc<RwLock<ProcessState>>,\n       is_active: Arc<AtomicBool>,\n   }\n   \n   impl ProcessManager {\n       fn check_state(&self) -> ProcessState {\n           self.state.read().unwrap().clone()\n       }\n       \n       fn update_state(&self, new_state: ProcessState) {\n           *self.state.write().unwrap() = new_state;\n       }\n   }\n   ```\n\n4) **Handle truly necessary unsafe code**:\n   - If interfacing with C libraries or system calls, wrap unsafe code in safe abstractions\n   - Document safety invariants with comments explaining:\n     - What assumptions the unsafe code makes\n     - What conditions must be met for the code to be safe\n     - Why safe alternatives cannot be used\n   - Example documentation:\n   ```rust\n   // SAFETY: The pointer `ptr` is guaranteed to be valid and aligned because:\n   // 1. It comes from a Box allocation which ensures proper alignment\n   // 2. We hold an exclusive lock preventing concurrent access\n   // 3. The lifetime 'a ensures the data outlives this function\n   unsafe {\n       // Minimal unsafe code here\n   }\n   ```\n\n5) **Create safe abstractions for system interactions**:\n   - If checking process status via system calls, use nix or libc crates with safe wrappers\n   - Implement error handling for all system operations\n   - Example safe wrapper:\n   ```rust\n   use nix::sys::wait::{waitpid, WaitStatus};\n   use nix::unistd::Pid;\n   \n   fn check_process_status(pid: i32) -> Result<ProcessStatus, Error> {\n       match waitpid(Pid::from_raw(pid), None) {\n           Ok(WaitStatus::Exited(_, code)) => Ok(ProcessStatus::Exited(code)),\n           Ok(WaitStatus::Signaled(_, sig, _)) => Ok(ProcessStatus::Signaled(sig)),\n           Ok(_) => Ok(ProcessStatus::Running),\n           Err(e) => Err(Error::SystemError(e)),\n       }\n   }\n   ```\n\n6) **Refactor concurrent access patterns**:\n   - Replace manual memory synchronization with channels or actors\n   - Use parking_lot for performance-critical locks if needed\n   - Implement timeout mechanisms to prevent deadlocks",
        "testStrategy": "Verify the unsafe code replacement with comprehensive testing:\n\n1) **Static analysis verification**:\n   - Run `grep -n \"unsafe\" src/sync.rs` before and after changes\n   - Verify significant reduction in unsafe blocks (target: 90%+ reduction)\n   - Use `cargo clippy` with pedantic lints to catch potential issues\n   - Run `cargo miri test` if applicable to detect undefined behavior\n\n2) **Unit tests for process state management**:\n   - Test concurrent access to process state from multiple threads\n   - Verify no data races occur under high contention\n   - Test state transitions are atomic and consistent\n   - Example test:\n   ```rust\n   #[test]\n   fn test_concurrent_state_updates() {\n       let manager = Arc::new(ProcessManager::new());\n       let handles: Vec<_> = (0..100).map(|i| {\n           let mgr = manager.clone();\n           thread::spawn(move || {\n               mgr.update_state(ProcessState::Running { pid: i });\n           })\n       }).collect();\n       \n       for handle in handles {\n           handle.join().unwrap();\n       }\n       \n       // Verify final state is valid\n   }\n   ```\n\n3) **Integration tests for process synchronization**:\n   - Spawn actual child processes and verify state tracking\n   - Test edge cases: process crashes, signals, zombie processes\n   - Verify no resource leaks occur over many iterations\n   - Test timeout handling and cleanup\n\n4) **Performance benchmarks**:\n   - Compare performance before and after unsafe removal\n   - Ensure synchronization overhead is acceptable\n   - Use criterion.rs for micro-benchmarks of critical paths\n   - Target: Less than 10% performance regression\n\n5) **Safety documentation review**:\n   - For any remaining unsafe blocks, verify comprehensive safety comments\n   - Ensure all invariants are documented and testable\n   - Review with another developer familiar with unsafe Rust\n\n6) **Stress testing**:\n   - Run the sync module under heavy load for extended periods\n   - Use tools like ThreadSanitizer to detect race conditions\n   - Monitor for panics, deadlocks, or resource exhaustion",
        "status": "done",
        "dependencies": [
          "30"
        ],
        "priority": "low",
        "subtasks": [],
        "updatedAt": "2026-01-23T12:01:56.488Z"
      },
      {
        "id": "32",
        "title": "Add comprehensive test coverage for security-affected areas",
        "description": "Create extensive test suite covering all security-critical functionality including CORS, origin validation, API authentication, path traversal, and rate limiting before implementing security fixes.",
        "details": "Create a comprehensive security test suite in tests/integration/security_tests.rs that establishes baseline behavior for all security-critical areas. This must be completed before any security hardening begins.\n\n**1. CORS Configuration Tests (for Task 22):**\n```rust\n#[cfg(test)]\nmod cors_tests {\n    use actix_web::{test, App, http::header};\n    \n    #[actix_web::test]\n    async fn test_cors_blocks_unauthorized_origins() {\n        // Test that requests from non-whitelisted origins are blocked\n        let app = test::init_service(create_app()).await;\n        let req = test::TestRequest::get()\n            .uri(\"/api/emails\")\n            .header(header::ORIGIN, \"https://evil.com\")\n            .to_request();\n        let resp = test::call_service(&app, req).await;\n        // Initially may pass (document current behavior)\n    }\n    \n    #[actix_web::test]\n    async fn test_cors_allows_configured_origins() {\n        // Test that ALLOWED_ORIGINS are properly accepted\n        std::env::set_var(\"ALLOWED_ORIGINS\", \"http://localhost:3000,http://localhost:5173\");\n        // Test requests from these origins succeed\n    }\n    \n    #[actix_web::test]\n    async fn test_preflight_options_requests() {\n        // Test OPTIONS preflight requests work correctly\n        // Check Access-Control headers in response\n    }\n    \n    #[actix_web::test]\n    async fn test_cors_credentials_mode() {\n        // Test Access-Control-Allow-Credentials header\n    }\n}\n```\n\n**2. Origin Validation Tests (for Task 23):**\n```rust\nmod origin_validation_tests {\n    #[actix_web::test]\n    async fn test_exact_origin_matching() {\n        // Test that \"evil.localhost.com\" doesn't match \"localhost.com\"\n        // Test substring matching is rejected\n    }\n    \n    #[actix_web::test]\n    async fn test_missing_origin_header() {\n        // Test requests without Origin header\n        // Document current behavior (may currently pass)\n    }\n    \n    #[actix_web::test]\n    async fn test_spoofed_origin_patterns() {\n        // Test various spoofing attempts:\n        // - \"localhost.evil.com\"\n        // - \"localhost.com.evil.com\"\n        // - \"localhost:3000.evil.com\"\n    }\n    \n    #[actix_web::test]\n    async fn test_port_number_validation() {\n        // Test that port numbers are validated\n        // \"localhost:3000\" != \"localhost:3001\"\n    }\n}\n```\n\n**3. API Key Authentication Tests (for Tasks 24, 25):**\n```rust\nmod api_auth_tests {\n    #[actix_web::test]\n    async fn test_mcp_endpoints_require_api_key() {\n        // Test all MCP endpoints reject requests without API key\n        let endpoints = vec![\n            \"/mcp/tools\",\n            \"/mcp/tools/list_emails/run\",\n            \"/mcp/tools/get_email/run\",\n            // ... all MCP endpoints\n        ];\n        \n        for endpoint in endpoints {\n            let req = test::TestRequest::post()\n                .uri(endpoint)\n                .to_request();\n            let resp = test::call_service(&app, req).await;\n            // Document current behavior\n        }\n    }\n    \n    #[actix_web::test]\n    async fn test_api_key_scope_enforcement() {\n        // Test that API keys with limited scopes are properly restricted\n        // Test read-only keys can't write\n        // Test MCP-only keys can't access REST endpoints\n    }\n    \n    #[actix_web::test]\n    async fn test_invalid_api_key_rejection() {\n        // Test expired keys, malformed keys, non-existent keys\n    }\n    \n    #[actix_web::test]\n    async fn test_no_test_credentials_seeded() {\n        // Verify database doesn't contain default test API keys\n        // Check for common test patterns: \"test\", \"demo\", \"example\"\n    }\n}\n```\n\n**4. Path Traversal Tests (for Task 27):**\n```rust\nmod path_traversal_tests {\n    #[actix_web::test]\n    async fn test_directory_traversal_patterns() {\n        // Test various traversal attempts:\n        let patterns = vec![\n            \"../../../etc/passwd\",\n            \"..\\\\..\\\\windows\\\\system32\",\n            \"%2e%2e%2f\",\n            \"..%252f\",\n            \"%c0%ae%c0%ae/\",\n            \"....//\",\n            \"..;/\",\n        ];\n        \n        for pattern in patterns {\n            // Test attachment download/upload with malicious paths\n            // Document current behavior\n        }\n    }\n    \n    #[actix_web::test]\n    async fn test_symlink_escape_attempts() {\n        // Create symlink pointing outside storage directory\n        // Test that following symlinks is blocked\n    }\n    \n    #[actix_web::test]\n    async fn test_path_canonicalization() {\n        // Test that paths are properly canonicalized\n        // Test relative paths are resolved\n    }\n    \n    #[actix_web::test]\n    async fn test_storage_directory_containment() {\n        // Verify all file operations stay within designated directory\n    }\n}\n```\n\n**5. Rate Limiting Tests (for Task 28):**\n```rust\nmod rate_limiting_tests {\n    #[actix_web::test]\n    async fn test_rest_api_rate_limits() {\n        // Test rate limiting on REST endpoints\n        // Make requests exceeding limit\n        // Verify 429 response\n    }\n    \n    #[actix_web::test]\n    async fn test_mcp_api_rate_limits() {\n        // Test rate limiting on MCP endpoints\n        // Ensure MCP routes are also protected\n    }\n    \n    #[actix_web::test]\n    async fn test_rate_limit_headers() {\n        // Check for X-RateLimit-Limit header\n        // Check for X-RateLimit-Remaining header\n        // Check for X-RateLimit-Reset header\n    }\n    \n    #[actix_web::test]\n    async fn test_rate_limit_429_response() {\n        // Verify proper 429 Too Many Requests response\n        // Check Retry-After header\n    }\n}\n```\n\n**Implementation Guidelines:**\n- Each test should first document CURRENT behavior (even if insecure)\n- Add clear comments indicating expected vs actual behavior\n- Tests should be designed to pass with current code\n- As security fixes are implemented, update tests to verify secure behavior\n- Use test fixtures and helper functions to reduce duplication\n- Include both positive and negative test cases\n- Test edge cases and boundary conditions",
        "testStrategy": "Verify comprehensive test coverage implementation:\n\n1. **Test File Creation:**\n   - Confirm tests/integration/security_tests.rs is created\n   - Verify all five test modules are present\n   - Check that tests compile without errors\n\n2. **CORS Tests Validation:**\n   - Run CORS tests and document current permissive behavior\n   - Verify tests check origin validation, preflight, and credentials\n   - Confirm tests are ready to validate Task 22 fixes\n\n3. **Origin Validation Tests:**\n   - Run origin tests documenting current behavior\n   - Verify exact matching tests (not substring)\n   - Confirm spoofing patterns are tested\n\n4. **API Authentication Tests:**\n   - Run auth tests on all MCP endpoints\n   - Document which endpoints currently lack authentication\n   - Verify scope enforcement tests are present\n\n5. **Path Traversal Tests:**\n   - Run traversal tests with various attack patterns\n   - Document current vulnerability status\n   - Verify symlink and canonicalization tests work\n\n6. **Rate Limiting Tests:**\n   - Run rate limit tests on both REST and MCP routes\n   - Document current rate limiting status\n   - Verify 429 response and header tests\n\n7. **Test Execution:**\n   ```bash\n   cargo test --test security_tests -- --nocapture\n   ```\n   - All tests should run (may fail documenting insecure behavior)\n   - Generate test report showing coverage gaps\n\n8. **Documentation Review:**\n   - Each test should have clear comments\n   - Current vs expected behavior documented\n   - Ready for updates as security fixes are applied",
        "status": "done",
        "dependencies": [],
        "priority": "high",
        "subtasks": [],
        "updatedAt": "2026-01-23T10:36:09.813Z"
      },
      {
        "id": "33",
        "title": "Add sampler configuration schema to database",
        "description": "Create a new database table 'ai_sampler_configs' to store per-model sampler settings including temperature, top_p, top_k, min_p, repeat_penalty, num_ctx (context window), think mode, stop sequences, and other provider-specific options.",
        "details": "Create a new database migration file migrations/005_create_ai_sampler_configs.sql with the following schema:\n\n```sql\nCREATE TABLE ai_sampler_configs (\n    id SERIAL PRIMARY KEY,\n    provider VARCHAR(255) NOT NULL,\n    model_name VARCHAR(255) NOT NULL,\n    temperature DECIMAL(3,2) DEFAULT 0.7,\n    top_p DECIMAL(3,2) DEFAULT 1.0,\n    top_k INTEGER DEFAULT NULL,\n    min_p DECIMAL(4,3) DEFAULT 0.01,\n    repeat_penalty DECIMAL(3,2) DEFAULT 1.0,\n    num_ctx INTEGER DEFAULT 2048,\n    think_mode BOOLEAN DEFAULT FALSE,\n    stop_sequences TEXT[] DEFAULT '{}',\n    provider_specific_options JSONB DEFAULT '{}',\n    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,\n    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,\n    UNIQUE(provider, model_name),\n    FOREIGN KEY (provider, model_name) REFERENCES ai_model_configurations(provider, model_name) ON DELETE CASCADE\n);\n\n-- Create trigger to update updated_at timestamp\nCREATE OR REPLACE FUNCTION update_updated_at_column()\nRETURNS TRIGGER AS $$\nBEGIN\n    NEW.updated_at = CURRENT_TIMESTAMP;\n    RETURN NEW;\nEND;\n$$ language 'plpgsql';\n\nCREATE TRIGGER update_ai_sampler_configs_updated_at BEFORE UPDATE\n    ON ai_sampler_configs FOR EACH ROW EXECUTE FUNCTION update_updated_at_column();\n\n-- Insert default configurations for known models\nINSERT INTO ai_sampler_configs (provider, model_name, temperature, top_p, min_p, repeat_penalty, num_ctx, think_mode, stop_sequences, provider_specific_options)\nVALUES \n    ('ollama', 'qwen2.5:7b', 0.7, 1.0, 0.01, 1.0, 8192, FALSE, '{}', '{}'),\n    ('ollama', 'llama3.3:70b', 0.7, 1.0, 0.01, 1.0, 8192, FALSE, '{}', '{}'),\n    ('openai', 'gpt-4', 0.7, 1.0, NULL, NULL, 8192, FALSE, '{}', '{\"presence_penalty\": 0, \"frequency_penalty\": 0}'),\n    ('anthropic', 'claude-3-opus', 0.7, 1.0, NULL, NULL, 200000, FALSE, '{}', '{}'),\n    ('glm', 'GLM-4.7-Flash', 0.7, 1.0, 0.01, 1.0, 51200, FALSE, '{}', '{}');\n```\n\nKey implementation considerations:\n1. Use DECIMAL types for floating-point sampler values to ensure precision\n2. Make top_k nullable since not all providers support it\n3. Use TEXT[] for stop_sequences to support multiple stop strings\n4. Use JSONB for provider_specific_options to allow flexible storage of provider-unique settings\n5. Include composite foreign key to ai_model_configurations table\n6. Add unique constraint on (provider, model_name) to ensure one config per model\n7. Include sensible defaults for common models with their known optimal settings\n8. Add timestamps for audit trail",
        "testStrategy": "1. Run the migration and verify table creation:\n   ```sql\n   \\d ai_sampler_configs\n   ```\n   Confirm all columns exist with correct types and constraints\n\n2. Test foreign key constraint:\n   - Try inserting a config for a non-existent model and verify it fails\n   - Insert a valid model in ai_model_configurations first, then add its sampler config\n\n3. Test unique constraint:\n   - Try inserting duplicate (provider, model_name) combinations and verify it fails\n\n4. Verify default values:\n   - Insert a row with minimal data and check that defaults are applied correctly\n\n5. Test the update trigger:\n   - Update a row and verify updated_at changes while created_at remains the same\n\n6. Query the default configurations:\n   ```sql\n   SELECT * FROM ai_sampler_configs ORDER BY provider, model_name;\n   ```\n   Verify all 5 default model configurations are present with correct values\n\n7. Test JSONB operations on provider_specific_options:\n   ```sql\n   UPDATE ai_sampler_configs \n   SET provider_specific_options = '{\"custom_param\": \"value\"}'::jsonb \n   WHERE provider = 'ollama' AND model_name = 'qwen2.5:7b';\n   ```\n\n8. Test cascade delete:\n   - Delete a model from ai_model_configurations and verify its sampler config is also deleted",
        "status": "done",
        "dependencies": [
          "1"
        ],
        "priority": "high",
        "subtasks": [],
        "updatedAt": "2026-01-24T19:35:58.183Z"
      },
      {
        "id": "34",
        "title": "Create Rust service layer for sampler configurations",
        "description": "Implement a SamplerConfigService in Rust that manages sampler configurations in the database, providing methods to get/save configs and integrate with existing AI provider adapters to apply sampler settings dynamically.",
        "details": "Create src/dashboard/services/ai/sampler_config.rs with the following components:\n\n1. Define SamplerConfiguration struct with fields:\n   - id: i64\n   - provider: String (ollama, llamacpp, etc.)\n   - model_name: String\n   - temperature: Option<f32>\n   - top_p: Option<f32>\n   - top_k: Option<i32>\n   - repeat_penalty: Option<f32>\n   - seed: Option<i64>\n   - max_tokens: Option<i32>\n   - stop_sequences: Option<Vec<String>>\n   - created_at: DateTime\n   - updated_at: DateTime\n\n2. Implement SamplerConfigService with methods:\n   - async fn get_config_for_model(provider: &str, model: &str) -> Result<Option<SamplerConfiguration>>\n   - async fn save_config(config: SamplerConfiguration) -> Result<SamplerConfiguration>\n   - async fn list_configs() -> Result<Vec<SamplerConfiguration>>\n   - async fn get_default_config_for_provider(provider: &str) -> Result<SamplerConfiguration>\n   - async fn delete_config(id: i64) -> Result<()>\n\n3. Create database migration migrations/005_create_sampler_configs.sql:\n   ```sql\n   CREATE TABLE sampler_configurations (\n       id INTEGER PRIMARY KEY AUTOINCREMENT,\n       provider TEXT NOT NULL,\n       model_name TEXT NOT NULL,\n       temperature REAL,\n       top_p REAL,\n       top_k INTEGER,\n       repeat_penalty REAL,\n       seed INTEGER,\n       max_tokens INTEGER,\n       stop_sequences TEXT, -- JSON array\n       created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n       updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n       UNIQUE(provider, model_name)\n   );\n   \n   -- Insert default configurations\n   INSERT INTO sampler_configurations (provider, model_name, temperature, top_p, top_k, max_tokens)\n   VALUES \n   ('ollama', 'default', 0.7, 0.9, 40, 2048),\n   ('llamacpp', 'default', 0.8, 0.95, 50, 4096);\n   ```\n\n4. Update existing AI provider adapters to use SamplerConfigService:\n   - Modify OllamaAdapter::generate_response() to fetch sampler config before API call\n   - Modify LlamaCppAdapter::generate_response() similarly\n   - Apply sampler settings to the request payload dynamically\n   - Fall back to provider defaults if no config found\n\n5. Add sampler config application logic:\n   ```rust\n   // In adapter's generate_response method\n   let sampler_config = self.sampler_service\n       .get_config_for_model(&self.provider, &model_name)\n       .await?\n       .or_else(|| self.sampler_service.get_default_config_for_provider(&self.provider).await.ok())?;\n   \n   // Apply to request\n   if let Some(temp) = sampler_config.temperature {\n       request.temperature = Some(temp);\n   }\n   // ... apply other settings\n   ```\n\n6. Ensure thread-safe access to SamplerConfigService using Arc<SamplerConfigService> in adapters.",
        "testStrategy": "1. Unit tests for SamplerConfigService:\n   - Test CRUD operations (create, read, update, delete configs)\n   - Test unique constraint on (provider, model_name)\n   - Test get_config_for_model returns correct config or None\n   - Test default config retrieval for each provider\n   - Test list_configs returns all configurations\n\n2. Integration tests with database:\n   - Create test database and run migration\n   - Verify default configs are inserted\n   - Test concurrent access patterns\n   - Test config updates reflect in subsequent reads\n\n3. Adapter integration tests:\n   - Mock SamplerConfigService in OllamaAdapter tests\n   - Verify generate_response applies sampler settings correctly\n   - Test fallback to defaults when no config exists\n   - Test that all sampler parameters are properly mapped to API request\n\n4. Manual testing:\n   - Create custom sampler config via direct DB insert\n   - Call generate_response and verify API logs show correct parameters\n   - Test with extreme values (temperature=0, temperature=2.0)\n   - Verify stop sequences are properly serialized/deserialized\n\n5. Performance testing:\n   - Measure overhead of config lookup on each generate_response call\n   - Consider caching strategy if lookup impacts performance",
        "status": "done",
        "dependencies": [
          "1",
          "2"
        ],
        "priority": "high",
        "subtasks": [],
        "updatedAt": "2026-01-24T19:39:45.270Z"
      },
      {
        "id": "35",
        "title": "Build WebUI sampler configuration panel",
        "description": "Create a React component in the dashboard settings page that allows users to configure sampler settings for each AI provider/model with provider-specific field visibility and test functionality.",
        "details": "Create src/dashboard/components/settings/SamplerConfigPanel.tsx with the following implementation:\n\n1. **Component Structure**:\n   ```tsx\n   interface SamplerConfig {\n     provider: string;\n     modelName: string;\n     temperature: number;\n     topP: number;\n     topK?: number;\n     minP?: number;\n     repeatPenalty: number;\n     numCtx: number;\n     thinkMode: boolean;\n     stopSequences: string[];\n   }\n   ```\n\n2. **Main Component Features**:\n   - Provider/Model selector dropdown that loads available models from the backend\n   - Temperature slider (0-2, step 0.1) with numeric display\n   - Top_p slider (0-1, step 0.01) with numeric display\n   - Top_k number input (optional, min 1)\n   - Min_p slider (0-1, step 0.001) - only shown for llama.cpp provider\n   - Repeat_penalty slider (0-2, step 0.1)\n   - Context window size (num_ctx) number input with provider-specific limits\n   - Think mode toggle switch\n   - Stop sequences text area with ability to add/remove sequences (one per line)\n\n3. **Provider-Specific Field Visibility**:\n   ```tsx\n   const providerFields = {\n     'ollama': ['temperature', 'topP', 'topK', 'repeatPenalty', 'numCtx', 'stopSequences'],\n     'llamacpp': ['temperature', 'topP', 'topK', 'minP', 'repeatPenalty', 'numCtx', 'stopSequences'],\n     'openai': ['temperature', 'topP', 'stopSequences', 'thinkMode']\n   };\n   ```\n\n4. **API Integration**:\n   - GET /api/sampler-configs/:provider/:model to load existing config\n   - POST /api/sampler-configs to save configuration\n   - GET /api/models to load available models per provider\n   - POST /api/sampler-configs/test to test configuration\n\n5. **Reset to Defaults Button**:\n   ```tsx\n   const defaultConfigs = {\n     'ollama': { temperature: 0.7, topP: 1.0, repeatPenalty: 1.0, numCtx: 2048 },\n     'llamacpp': { temperature: 0.7, topP: 1.0, minP: 0.01, repeatPenalty: 1.0, numCtx: 2048 },\n     'openai': { temperature: 0.7, topP: 1.0, thinkMode: false }\n   };\n   ```\n\n6. **Test Configuration Feature**:\n   - Modal dialog with test prompt input\n   - Sends request to backend with current config + test prompt\n   - Displays response and timing information\n   - Shows any errors or warnings\n\n7. **UI Components**:\n   - Use Material-UI or existing design system components\n   - Tooltips for each setting explaining its purpose\n   - Visual feedback for saving (loading spinner, success toast)\n   - Validation feedback (red borders for invalid values)\n\n8. **State Management**:\n   - Use React hooks (useState, useEffect) for local state\n   - Debounce slider changes to avoid excessive API calls\n   - Show unsaved changes indicator\n   - Confirm navigation away with unsaved changes",
        "testStrategy": "1. **Component Rendering Tests**:\n   - Verify component renders without errors\n   - Check all form fields are present for default provider\n   - Verify provider-specific fields show/hide correctly when switching providers\n   - Test that switching models loads the correct configuration\n\n2. **Field Interaction Tests**:\n   - Test temperature slider updates value correctly (0-2 range)\n   - Test top_p slider updates value correctly (0-1 range)\n   - Verify min_p field only appears for llama.cpp provider\n   - Test stop sequences can be added/removed\n   - Verify number inputs reject invalid values\n\n3. **API Integration Tests**:\n   - Mock GET /api/sampler-configs/:provider/:model and verify config loads\n   - Mock POST /api/sampler-configs and verify save functionality\n   - Test error handling for failed API calls\n   - Verify loading states display correctly\n\n4. **Reset Functionality Test**:\n   - Click \"Reset to Defaults\" and verify all fields update\n   - Ensure provider-specific defaults are applied correctly\n   - Verify unsaved changes indicator appears after reset\n\n5. **Test Configuration Feature**:\n   - Click \"Test Configuration\" and verify modal opens\n   - Enter test prompt and submit\n   - Mock POST /api/sampler-configs/test endpoint\n   - Verify response displays in modal\n   - Test error handling for failed test requests\n\n6. **Validation Tests**:\n   - Enter invalid values (negative numbers, out of range)\n   - Verify validation messages appear\n   - Ensure save is disabled with invalid values\n   - Test required field validation\n\n7. **Integration Test**:\n   - Load existing configuration\n   - Modify multiple fields\n   - Save configuration\n   - Reload page and verify changes persisted\n   - Test configuration with actual prompt",
        "status": "done",
        "dependencies": [
          "33",
          "34"
        ],
        "priority": "high",
        "subtasks": [],
        "updatedAt": "2026-01-24T19:45:28.706Z"
      },
      {
        "id": "36",
        "title": "Add sensible default sampler presets for common models",
        "description": "Research and implement default sampler configurations for popular models used with local inference, creating presets for GLM-4.7-Flash, Qwen, Llama, Mistral models and generic defaults for cloud providers, storing them as seed data in the database with documentation.",
        "details": "Create a database migration to seed the ai_sampler_configs table with well-researched default configurations:\n\n1. **Create migration file** migrations/006_seed_default_sampler_configs.sql:\n\n```sql\n-- GLM-4.7-Flash defaults (optimized for fast, coherent responses)\nINSERT INTO ai_sampler_configs (provider, model_name, temperature, top_p, min_p, repeat_penalty, num_ctx, think_mode, created_at, updated_at)\nVALUES \n    ('ollama', 'glm4:7b-flash', 0.7, 1.0, 0.01, 1.0, 51200, false, NOW(), NOW()),\n    -- Temperature 0.7: Balanced creativity without excessive randomness\n    -- top_p 1.0: No nucleus sampling restriction, model can use full vocabulary\n    -- min_p 0.01: Filters out tokens with <1% probability to reduce noise\n    -- repeat_penalty 1.0: No penalty, GLM models handle repetition well internally\n    -- num_ctx 51200: Large context window for complex email threads\n    -- think_mode false: Flash variant optimized for speed, not deep reasoning\n\n    -- Qwen models (optimized for instruction following)\n    ('ollama', 'qwen2.5:7b', 0.7, 0.95, 0.05, 1.1, 32768, false, NOW(), NOW()),\n    ('ollama', 'qwen2.5:14b', 0.7, 0.95, 0.05, 1.1, 32768, false, NOW(), NOW()),\n    ('ollama', 'qwen2.5:32b', 0.7, 0.95, 0.05, 1.1, 32768, false, NOW(), NOW()),\n    -- Temperature 0.7: Standard for instruction-following tasks\n    -- top_p 0.95: Slight nucleus sampling for more focused outputs\n    -- min_p 0.05: Higher threshold to ensure quality token selection\n    -- repeat_penalty 1.1: Slight penalty to encourage variety\n    -- num_ctx 32768: Qwen's native context size\n    -- think_mode false: Qwen models don't support CoT prompting\n\n    -- Llama 3.x models (optimized for general purpose)\n    ('ollama', 'llama3.2:3b', 0.8, 0.9, 0.05, 1.15, 8192, false, NOW(), NOW()),\n    ('ollama', 'llama3.2:7b', 0.8, 0.9, 0.05, 1.15, 8192, false, NOW(), NOW()),\n    ('ollama', 'llama3.1:70b', 0.7, 0.9, 0.05, 1.1, 131072, false, NOW(), NOW()),\n    -- Temperature 0.8/0.7: Higher for smaller models, lower for larger\n    -- top_p 0.9: Moderate nucleus sampling for coherence\n    -- min_p 0.05: Standard quality threshold\n    -- repeat_penalty 1.15/1.1: Higher for smaller models to reduce loops\n    -- num_ctx: Model-specific limits\n    \n    -- Mistral models (optimized for reasoning)\n    ('ollama', 'mistral:7b', 0.7, 0.95, 0.05, 1.1, 32768, false, NOW(), NOW()),\n    ('ollama', 'mixtral:8x7b', 0.7, 0.95, 0.05, 1.1, 32768, false, NOW(), NOW()),\n    ('ollama', 'mistral-large:123b', 0.6, 0.95, 0.05, 1.05, 32768, false, NOW(), NOW()),\n    -- Temperature 0.6-0.7: Lower for larger models\n    -- top_p 0.95: Consistent nucleus sampling\n    -- repeat_penalty: Lower for larger models\n    \n    -- Generic cloud provider defaults\n    ('openai', 'gpt-4-turbo', 0.7, 1.0, NULL, NULL, 128000, false, NOW(), NOW()),\n    ('openai', 'gpt-4o', 0.7, 1.0, NULL, NULL, 128000, false, NOW(), NOW()),\n    ('openai', 'gpt-3.5-turbo', 0.7, 1.0, NULL, NULL, 16384, false, NOW(), NOW()),\n    -- OpenAI models: Only temperature and top_p supported\n    -- num_ctx matches model's context window\n    \n    ('anthropic', 'claude-3-opus', 0.7, 1.0, NULL, NULL, 200000, false, NOW(), NOW()),\n    ('anthropic', 'claude-3-sonnet', 0.7, 1.0, NULL, NULL, 200000, false, NOW(), NOW()),\n    ('anthropic', 'claude-3-haiku', 0.7, 1.0, NULL, NULL, 200000, false, NOW(), NOW()),\n    -- Anthropic: Similar to OpenAI, large context windows\n    \n    -- LlamaCpp defaults (for self-hosted models)\n    ('llamacpp', 'default', 0.7, 0.95, 0.05, 1.1, 4096, false, NOW(), NOW());\n    -- Conservative defaults for unknown models\n```\n\n2. **Add rollback migration** for reversibility:\n```sql\n-- Rollback: Remove only the seeded defaults, preserve user configurations\nDELETE FROM ai_sampler_configs \nWHERE created_at = updated_at \nAND model_name IN (\n    'glm4:7b-flash', 'qwen2.5:7b', 'qwen2.5:14b', 'qwen2.5:32b',\n    'llama3.2:3b', 'llama3.2:7b', 'llama3.1:70b',\n    'mistral:7b', 'mixtral:8x7b', 'mistral-large:123b',\n    'gpt-4-turbo', 'gpt-4o', 'gpt-3.5-turbo',\n    'claude-3-opus', 'claude-3-sonnet', 'claude-3-haiku',\n    'default'\n);\n```\n\n3. **Create documentation file** docs/sampler-presets.md explaining the rationale:\n```markdown\n# Default Sampler Configurations\n\n## Overview\nThis document explains the default sampler settings for various AI models.\n\n## Parameter Explanations\n\n### Temperature (0.0 - 2.0)\n- Controls randomness in token selection\n- 0.0: Deterministic (always picks most likely token)\n- 0.7-0.8: Balanced creativity for general tasks\n- 1.0+: High creativity, may reduce coherence\n\n### Top-p (0.0 - 1.0)\n- Nucleus sampling: only consider tokens whose cumulative probability < top_p\n- 1.0: Consider all tokens\n- 0.9-0.95: Moderate filtering of unlikely tokens\n- <0.9: More focused, deterministic outputs\n\n### Min-p (0.0 - 1.0)\n- Minimum probability threshold for token consideration\n- Filters out tokens with probability < min_p\n- 0.01-0.05: Standard range for quality filtering\n\n### Repeat Penalty (1.0 - 2.0)\n- Penalizes tokens that have appeared recently\n- 1.0: No penalty\n- 1.1-1.15: Light penalty for variety\n- 1.5+: Strong penalty, may harm coherence\n\n## Model-Specific Rationales\n[Include detailed explanations for each model family]\n```\n\n4. **Update SamplerConfigService** to include a method for resetting to defaults:\n```rust\nimpl SamplerConfigService {\n    pub async fn reset_to_default(&self, provider: &str, model_name: &str) -> Result<()> {\n        // Query the seeded defaults where created_at = updated_at\n        // This identifies unchanged default configurations\n    }\n}\n```",
        "testStrategy": "1. **Migration Testing**:\n   - Run migration: `diesel migration run`\n   - Verify all default configurations are inserted:\n     ```sql\n     SELECT provider, model_name, temperature, top_p, min_p, repeat_penalty, num_ctx \n     FROM ai_sampler_configs \n     ORDER BY provider, model_name;\n     ```\n   - Confirm 17 rows inserted with correct values\n   - Test rollback removes only seeded data\n\n2. **Configuration Validation**:\n   - For each model, verify sampler settings are within valid ranges\n   - Test that NULL values are properly set for cloud providers (min_p, repeat_penalty)\n   - Verify num_ctx values match documented model limits\n\n3. **Integration Testing**:\n   - Start application and verify SamplerConfigService loads defaults\n   - Test get_config_for_model() returns correct preset for each model\n   - Verify WebUI displays presets correctly in dropdown\n   - Test that user modifications don't affect default lookup\n\n4. **Model Testing** (manual):\n   - For each model type, generate a test email draft\n   - Verify output quality matches expectations:\n     - GLM-4.7-Flash: Fast, coherent responses\n     - Qwen: Good instruction following\n     - Llama: Balanced general purpose output\n     - Mistral: Strong reasoning capability\n   - Compare outputs with/without presets to validate improvements\n\n5. **Documentation Verification**:\n   - Review docs/sampler-presets.md for accuracy\n   - Ensure all parameter choices are justified\n   - Verify model-specific notes are comprehensive",
        "status": "done",
        "dependencies": [
          "33",
          "34"
        ],
        "priority": "medium",
        "subtasks": [],
        "updatedAt": "2026-01-24T20:11:35.462Z"
      },
      {
        "id": "37",
        "title": "Add LM Studio provider adapter",
        "description": "Create a new LM Studio adapter similar to the llama.cpp adapter since LM Studio uses an OpenAI-compatible API but may have specific quirks. Support the same sampler settings as llama.cpp (min_p, top_no, etc.).",
        "details": "Create src/dashboard/services/ai/providers/lmstudio.rs with the following implementation:\n\n1. Define LMStudioProvider struct that implements the AI provider trait:\n   - base_url: String (from LMSTUDIO_BASE_URL env var, default: \"http://localhost:1234\")\n   - api_key: Option<String> (LM Studio typically doesn't require auth)\n   - client: reqwest::Client\n\n2. Implement provider methods:\n   - new() - Initialize with base URL from env or default\n   - complete() - Send completion requests to /v1/chat/completions endpoint\n   - complete_with_tools() - Send tool-enabled requests (if LM Studio supports it)\n   - list_models() - Query /v1/models endpoint\n\n3. Support LM Studio-specific sampler parameters in requests:\n   - temperature, top_p, top_k (standard OpenAI params)\n   - min_p (minimum probability threshold)\n   - top_a (top-a sampling)\n   - typical_p (typical sampling)\n   - tfs_z (tail-free sampling)\n   - repeat_penalty\n   - repeat_last_n\n   - penalize_nl\n   - presence_penalty\n   - frequency_penalty\n   - mirostat, mirostat_tau, mirostat_eta\n   - seed\n   - stop sequences\n\n4. Handle LM Studio response format quirks:\n   - Parse streaming responses if different from OpenAI\n   - Handle any non-standard error formats\n   - Support both streaming and non-streaming modes\n\n5. Add provider registration:\n   - Update provider factory/registry to include \"lmstudio\" as a provider option\n   - Ensure SamplerConfigService can apply LM Studio-specific samplers\n\n6. Environment variable support:\n   - LMSTUDIO_BASE_URL (default: http://localhost:1234)\n   - LMSTUDIO_API_KEY (optional, for future compatibility)\n\n7. Error handling:\n   - Connection errors when LM Studio isn't running\n   - Model not loaded errors\n   - Invalid sampler parameter combinations\n   - Timeout handling for slow local models\n\nExample implementation structure:\n```rust\nuse crate::dashboard::services::ai::{AIProvider, CompletionRequest, CompletionResponse};\n\npub struct LMStudioProvider {\n    base_url: String,\n    client: reqwest::Client,\n}\n\nimpl LMStudioProvider {\n    pub fn new() -> Result<Self, Box<dyn std::error::Error>> {\n        let base_url = std::env::var(\"LMSTUDIO_BASE_URL\")\n            .unwrap_or_else(|_| \"http://localhost:1234\".to_string());\n        \n        Ok(Self {\n            base_url,\n            client: reqwest::Client::new(),\n        })\n    }\n    \n    fn apply_sampler_config(&self, request: &mut serde_json::Value, config: &SamplerConfiguration) {\n        // Apply LM Studio specific samplers\n        if let Some(min_p) = config.min_p {\n            request[\"min_p\"] = json!(min_p);\n        }\n        // ... other samplers\n    }\n}\n\nimpl AIProvider for LMStudioProvider {\n    async fn complete(&self, request: CompletionRequest) -> Result<CompletionResponse, Box<dyn std::error::Error>> {\n        // Implementation\n    }\n}\n```",
        "testStrategy": "1. Unit tests for LMStudioProvider:\n   - Test initialization with default and custom LMSTUDIO_BASE_URL\n   - Mock HTTP responses for completion requests\n   - Test all sampler parameters are correctly included in requests\n   - Test error handling for connection failures\n   - Test streaming vs non-streaming response parsing\n\n2. Integration tests (requires LM Studio running):\n   - Test actual completion requests with a loaded model\n   - Verify sampler parameters affect output (e.g., temperature 0 vs 1)\n   - Test model listing endpoint\n   - Test timeout handling with large prompts\n   - Test special characters and Unicode handling\n\n3. Manual testing checklist:\n   - Install LM Studio on Windows/macOS\n   - Load a model (e.g., Llama 3.3)\n   - Configure RustyMail to use lmstudio provider\n   - Test email drafting with various sampler settings\n   - Verify min_p, top_a, and other LM Studio-specific samplers work\n   - Test with different model architectures (Llama, Mistral, Qwen)\n   - Test error messages when LM Studio isn't running\n   - Verify performance with local models\n\n4. Compatibility testing:\n   - Test with latest LM Studio version\n   - Verify OpenAI compatibility layer works as expected\n   - Test any LM Studio-specific extensions or deviations\n   - Document any limitations or unsupported features",
        "status": "done",
        "dependencies": [
          "34"
        ],
        "priority": "medium",
        "subtasks": [],
        "updatedAt": "2026-01-24T20:16:36.692Z"
      },
      {
        "id": "38",
        "title": "Wire sampler configuration from database to provider adapters",
        "description": "Modify all AI provider adapters (OllamaAdapter, LlamaAdapter, LMStudioAdapter, etc.) to fetch and apply sampler configurations from the database instead of using hardcoded defaults, implementing a priority system: database > environment variables > code defaults.",
        "details": "Update all provider adapters to integrate with the SamplerConfigService and apply configurations dynamically:\n\n1. **Update OllamaAdapter** (src/dashboard/services/ai/providers/ollama.rs):\n   ```rust\n   // Add sampler_config_service to the struct\n   struct OllamaAdapter {\n       base_url: String,\n       sampler_config_service: Arc<SamplerConfigService>,\n       // ... existing fields\n   }\n   \n   // In the complete() method, before making the request:\n   async fn complete(&self, request: CompletionRequest) -> Result<CompletionResponse> {\n       // Fetch sampler config from database\n       let sampler_config = self.sampler_config_service\n           .get_config_for_model(\"ollama\", &request.model)\n           .await?;\n       \n       // Build options with priority: DB > env > defaults\n       let mut options = json!({});\n       \n       if let Some(config) = sampler_config {\n           if let Some(temp) = config.temperature {\n               options[\"temperature\"] = json!(temp);\n           }\n           if let Some(top_p) = config.top_p {\n               options[\"top_p\"] = json!(top_p);\n           }\n           if let Some(top_k) = config.top_k {\n               options[\"top_k\"] = json!(top_k);\n           }\n           if let Some(repeat_penalty) = config.repeat_penalty {\n               options[\"repeat_penalty\"] = json!(repeat_penalty);\n           }\n           if let Some(num_ctx) = config.num_ctx {\n               options[\"num_ctx\"] = json!(num_ctx);\n           }\n           // Apply other sampler settings...\n       } else {\n           // Fall back to env vars or hardcoded defaults\n           options[\"temperature\"] = json!(env::var(\"OLLAMA_TEMPERATURE\")\n               .ok()\n               .and_then(|v| v.parse::<f32>().ok())\n               .unwrap_or(0.7));\n           // ... other defaults\n       }\n       \n       let ollama_request = json!({\n           \"model\": request.model,\n           \"prompt\": request.prompt,\n           \"options\": options,\n           \"stream\": request.stream.unwrap_or(false)\n       });\n       \n       // Make the API request...\n   }\n   ```\n\n2. **Update LlamaAdapter** (src/dashboard/services/ai/providers/llamacpp.rs):\n   ```rust\n   // Similar pattern but with llama.cpp specific parameters\n   let sampler_config = self.sampler_config_service\n       .get_config_for_model(\"llamacpp\", &request.model)\n       .await?;\n   \n   let mut body = json!({\n       \"prompt\": request.prompt,\n       \"n_predict\": request.max_tokens.unwrap_or(2048),\n       \"stream\": request.stream.unwrap_or(false)\n   });\n   \n   if let Some(config) = sampler_config {\n       // Apply llama.cpp specific parameters\n       if let Some(temp) = config.temperature {\n           body[\"temperature\"] = json!(temp);\n       }\n       if let Some(min_p) = config.min_p {\n           body[\"min_p\"] = json!(min_p);\n       }\n       if let Some(top_k) = config.top_k {\n           body[\"top_k\"] = json!(top_k);\n       }\n       // Handle stop sequences\n       if let Some(stop) = config.stop_sequences {\n           body[\"stop\"] = json!(stop);\n       }\n   }\n   ```\n\n3. **Update LMStudioAdapter** (src/dashboard/services/ai/providers/lmstudio.rs):\n   ```rust\n   // LM Studio uses OpenAI-compatible format but supports additional samplers\n   let sampler_config = self.sampler_config_service\n       .get_config_for_model(\"lmstudio\", &request.model)\n       .await?;\n   \n   // Build request with sampler settings\n   let mut lm_request = json!({\n       \"model\": request.model,\n       \"messages\": messages,\n       \"stream\": request.stream.unwrap_or(false)\n   });\n   \n   if let Some(config) = sampler_config {\n       // Apply OpenAI-style parameters\n       if let Some(temp) = config.temperature {\n           lm_request[\"temperature\"] = json!(temp);\n       }\n       if let Some(top_p) = config.top_p {\n           lm_request[\"top_p\"] = json!(top_p);\n       }\n       // LM Studio specific extensions\n       if let Some(min_p) = config.min_p {\n           lm_request[\"min_p\"] = json!(min_p);\n       }\n       if let Some(repeat_penalty) = config.repeat_penalty {\n           lm_request[\"frequency_penalty\"] = json!(repeat_penalty);\n       }\n   }\n   ```\n\n4. **Update provider factory** to inject SamplerConfigService:\n   ```rust\n   // In src/dashboard/services/ai/provider_factory.rs\n   pub fn create_provider(\n       provider_type: &str,\n       sampler_config_service: Arc<SamplerConfigService>,\n       // ... other params\n   ) -> Result<Box<dyn AIProvider>> {\n       match provider_type {\n           \"ollama\" => Ok(Box::new(OllamaAdapter::new(sampler_config_service)?)),\n           \"llamacpp\" => Ok(Box::new(LlamaAdapter::new(sampler_config_service)?)),\n           \"lmstudio\" => Ok(Box::new(LMStudioAdapter::new(sampler_config_service)?)),\n           _ => Err(anyhow!(\"Unknown provider type: {}\", provider_type))\n       }\n   }\n   ```\n\n5. **Add configuration caching** to reduce database queries:\n   ```rust\n   // In SamplerConfigService, add a cache layer\n   struct SamplerConfigService {\n       db_pool: Arc<DbPool>,\n       cache: Arc<RwLock<HashMap<(String, String), CachedConfig>>>,\n   }\n   \n   struct CachedConfig {\n       config: Option<SamplerConfiguration>,\n       expires_at: Instant,\n   }\n   \n   impl SamplerConfigService {\n       pub async fn get_config_for_model_cached(\n           &self,\n           provider: &str,\n           model_name: &str\n       ) -> Result<Option<SamplerConfiguration>> {\n           let key = (provider.to_string(), model_name.to_string());\n           \n           // Check cache first\n           {\n               let cache = self.cache.read().await;\n               if let Some(cached) = cache.get(&key) {\n                   if cached.expires_at > Instant::now() {\n                       return Ok(cached.config.clone());\n                   }\n               }\n           }\n           \n           // Fetch from database\n           let config = self.get_config_for_model(provider, model_name).await?;\n           \n           // Update cache\n           {\n               let mut cache = self.cache.write().await;\n               cache.insert(key, CachedConfig {\n                   config: config.clone(),\n                   expires_at: Instant::now() + Duration::from_secs(300), // 5 min cache\n               });\n           }\n           \n           Ok(config)\n       }\n   }\n   ```\n\n6. **Handle provider-specific parameter mappings**:\n   ```rust\n   // Create a trait for parameter mapping\n   trait SamplerParameterMapper {\n       fn map_config_to_request(&self, config: &SamplerConfiguration) -> serde_json::Value;\n   }\n   \n   // Implement for each provider with their specific parameter names\n   impl SamplerParameterMapper for OllamaAdapter {\n       fn map_config_to_request(&self, config: &SamplerConfiguration) -> serde_json::Value {\n           let mut params = json!({});\n           // Ollama uses \"num_ctx\" for context window\n           if let Some(num_ctx) = config.num_ctx {\n               params[\"num_ctx\"] = json!(num_ctx);\n           }\n           // ... map other parameters\n           params\n       }\n   }\n   ```",
        "testStrategy": "1. **Integration Tests for Each Provider**:\n   - Create test configurations in the database for each provider\n   - Mock provider API endpoints to capture the actual requests being sent\n   - Verify that sampler parameters from the database are correctly applied to requests\n   - Test fallback to environment variables when no DB config exists\n   - Test fallback to hardcoded defaults when neither DB nor env configs exist\n\n2. **Priority Order Testing**:\n   ```rust\n   #[tokio::test]\n   async fn test_config_priority_order() {\n       // Set up: Create DB config with temperature=0.5\n       let config = SamplerConfiguration {\n           provider: \"ollama\".to_string(),\n           model_name: \"llama2:7b\".to_string(),\n           temperature: Some(0.5),\n           // ...\n       };\n       sampler_service.save_config(config).await.unwrap();\n       \n       // Set environment variable to temperature=0.8\n       env::set_var(\"OLLAMA_TEMPERATURE\", \"0.8\");\n       \n       // Make completion request\n       let response = ollama_adapter.complete(request).await.unwrap();\n       \n       // Verify DB value (0.5) was used, not env value (0.8)\n       assert_eq!(captured_request[\"options\"][\"temperature\"], 0.5);\n   }\n   ```\n\n3. **Cache Performance Testing**:\n   - Measure database query count before and after implementing cache\n   - Verify cache expiration works correctly\n   - Test cache invalidation when configurations are updated\n   - Load test with multiple concurrent requests to ensure cache thread safety\n\n4. **Provider-Specific Parameter Testing**:\n   - Test Ollama-specific parameters (num_ctx, mirostat, etc.)\n   - Test llama.cpp-specific parameters (min_p, top_k, grammar, etc.)\n   - Test LM Studio OpenAI-compatible format with extensions\n   - Verify stop sequences are correctly formatted for each provider\n\n5. **Error Handling Tests**:\n   - Test behavior when database is unavailable (should fall back gracefully)\n   - Test invalid configuration values (e.g., temperature > 2.0)\n   - Test missing required parameters\n   - Verify error messages are helpful for debugging\n\n6. **End-to-End Testing**:\n   - Configure different sampler settings via the WebUI\n   - Make actual completion requests to each provider\n   - Verify response quality changes based on sampler settings\n   - Test with real models to ensure parameters are having expected effects",
        "status": "done",
        "dependencies": [],
        "priority": "high",
        "subtasks": [],
        "updatedAt": "2026-01-25T11:35:21.231Z"
      },
      {
        "id": "39",
        "title": "Persist Email Assistant provider/model selection to database",
        "description": "Save TopBar provider selection from in-memory ProviderManager to ai_model_configurations table with role='chatbot' and load on service startup to persist user's Email Assistant model choice across restarts.",
        "details": "Modify the Email Assistant to persist provider/model selection to the database instead of only storing in-memory:\n\n1. **Update ai_model_configurations table usage**:\n   - Use role='chatbot' for Email Assistant provider/model selection\n   - Store provider name, model name, and any provider-specific settings\n   - Ensure compatibility with existing 'tool_calling' and 'drafting' roles\n\n2. **Modify ProviderManager** (src/dashboard/services/ai/provider_manager.rs):\n   ```rust\n   // Add model_config_service dependency\n   struct ProviderManager {\n       providers: Arc<RwLock<HashMap<String, Box<dyn AIProvider>>>>,\n       current_provider: Arc<RwLock<String>>,\n       model_config_service: Arc<ModelConfigService>,\n   }\n   \n   // Update set_current_provider to persist to database\n   pub async fn set_current_provider(&self, provider_name: String, model_name: String) -> Result<()> {\n       // Update in-memory state\n       let mut current = self.current_provider.write().await;\n       *current = provider_name.clone();\n       \n       // Persist to database\n       let config = ModelConfiguration {\n           role: \"chatbot\".to_string(),\n           provider: provider_name,\n           model_name,\n           base_url: None, // Provider-specific, set if needed\n           api_key: None,  // Provider-specific, set if needed\n           additional_config: None,\n       };\n       \n       self.model_config_service.set_model_config(config).await?;\n       Ok(())\n   }\n   ```\n\n3. **Add startup initialization**:\n   ```rust\n   // In ProviderManager::new() or init()\n   pub async fn init(&self) -> Result<()> {\n       // Load saved chatbot configuration\n       if let Some(config) = self.model_config_service\n           .get_model_config(\"chatbot\").await? {\n           \n           // Set current provider from database\n           let mut current = self.current_provider.write().await;\n           *current = config.provider.clone();\n           \n           // Optionally validate provider exists\n           let providers = self.providers.read().await;\n           if !providers.contains_key(&config.provider) {\n               warn!(\"Saved provider {} not available, using default\", config.provider);\n               *current = \"ollama\".to_string(); // Or other default\n           }\n       }\n       Ok(())\n   }\n   ```\n\n4. **Update TopBar component integration**:\n   - Ensure TopBar calls the updated set_current_provider method\n   - Pass both provider and model name when selection changes\n   - Handle any UI state updates after persistence\n\n5. **Migration considerations**:\n   - Check if 'chatbot' role already exists in ai_model_configurations\n   - Handle upgrade path for users with existing in-memory selections\n   - Provide sensible defaults if no configuration exists\n\n6. **Error handling**:\n   - Gracefully handle database write failures (continue with in-memory)\n   - Log persistence errors without breaking provider switching\n   - Implement retry logic for transient database errors",
        "testStrategy": "1. **Unit tests for persistence logic**:\n   - Mock ModelConfigService and verify set_model_config is called with correct parameters\n   - Test that set_current_provider updates both in-memory and database state\n   - Verify error handling when database write fails\n\n2. **Integration tests for startup loading**:\n   - Insert test configuration with role='chatbot' into database\n   - Initialize ProviderManager and verify it loads the saved provider\n   - Test fallback behavior when saved provider doesn't exist\n   - Verify no configuration scenario uses appropriate defaults\n\n3. **End-to-end testing**:\n   - Start service and select a provider/model in TopBar\n   - Restart service and verify selection is preserved\n   - Test switching between different providers and models\n   - Verify other roles ('tool_calling', 'drafting') are not affected\n\n4. **Database verification**:\n   ```sql\n   -- Check chatbot configuration is saved\n   SELECT * FROM ai_model_configurations WHERE role = 'chatbot';\n   \n   -- Verify only one chatbot configuration exists\n   SELECT COUNT(*) FROM ai_model_configurations WHERE role = 'chatbot';\n   ```\n\n5. **Concurrency testing**:\n   - Simulate multiple provider switches in quick succession\n   - Verify final database state matches last selection\n   - Test simultaneous reads during provider switch",
        "status": "done",
        "dependencies": [],
        "priority": "high",
        "subtasks": [],
        "updatedAt": "2026-01-25T11:39:50.376Z"
      },
      {
        "id": "40",
        "title": "Extend agent_executor tool-calling to support all configured providers",
        "description": "Modify agent_executor.rs to support tool calling for all AI providers (OpenAI, Anthropic, etc.) beyond just Ollama, or restrict the UI to only show supported providers to prevent runtime errors.",
        "details": "Fix the limitation in src/dashboard/services/ai/agent_executor.rs line 189 where only 'ollama' provider is handled for tool calling:\n\n1. **Analyze current implementation**:\n   - Review the existing execute_with_tools() method that currently only supports Ollama\n   - Identify the tool-calling format differences between providers (Ollama uses custom format, OpenAI uses function calling, Anthropic uses tools API)\n   - Check how model configurations are loaded and which providers are configured\n\n2. **Implement provider-specific tool calling adapters**:\n   - Create a ToolCallingAdapter trait in agent_executor.rs with methods:\n     ```rust\n     trait ToolCallingAdapter {\n         fn format_tools(&self, tools: &[Tool]) -> Value;\n         fn parse_tool_calls(&self, response: &Value) -> Result<Vec<ToolCall>>;\n         fn format_tool_results(&self, results: &[ToolResult]) -> Value;\n     }\n     ```\n   - Implement OllamaToolAdapter (refactor existing code)\n   - Implement OpenAIToolAdapter for OpenAI function calling format\n   - Implement AnthropicToolAdapter for Anthropic tools API format\n   - Add placeholder adapters for other providers that return \"not supported\" error\n\n3. **Update AgentExecutor to use adapters**:\n   - Modify execute_with_tools() to select appropriate adapter based on provider:\n     ```rust\n     let adapter: Box<dyn ToolCallingAdapter> = match provider.as_str() {\n         \"ollama\" => Box::new(OllamaToolAdapter::new()),\n         \"openai\" => Box::new(OpenAIToolAdapter::new()),\n         \"anthropic\" => Box::new(AnthropicToolAdapter::new()),\n         _ => return Err(\"Unsupported tool-calling provider\")\n     };\n     ```\n   - Use adapter methods to format tools, parse responses, and format results\n   - Ensure error handling covers provider-specific edge cases\n\n4. **Add provider capability detection**:\n   - Create a supports_tool_calling() method in model configurations\n   - Add a tool_calling_capable boolean field to provider configs\n   - Update get_model_configurations tool to include this capability flag\n\n5. **Update UI/API to respect capabilities**:\n   - Modify the model selection endpoints to filter out non-tool-calling models when selecting tool-calling model\n   - Add validation in set_tool_calling_model to reject unsupported providers\n   - Return clear error messages when attempting to use unsupported providers\n\n6. **Handle provider-specific nuances**:\n   - OpenAI: Convert MCP tools to OpenAI function schema, handle function_call responses\n   - Anthropic: Format tools according to Anthropic's tool use format, parse tool_use blocks\n   - Add appropriate headers and API version parameters for each provider\n   - Handle streaming vs non-streaming responses appropriately",
        "testStrategy": "Verify multi-provider tool calling support with comprehensive testing:\n\n1. **Unit tests for tool adapters**:\n   - Test OllamaToolAdapter formats tools correctly and parses Ollama-style responses\n   - Test OpenAIToolAdapter converts MCP tools to OpenAI function schema\n   - Test AnthropicToolAdapter formats according to Anthropic tool use spec\n   - Verify error handling for malformed responses from each provider\n\n2. **Integration tests for AgentExecutor**:\n   - Mock responses from different providers and verify correct tool execution flow\n   - Test provider selection logic with various model configurations\n   - Verify fallback behavior for unsupported providers\n   - Test error propagation when provider returns unexpected format\n\n3. **End-to-end testing**:\n   - Test process_email_instructions with Ollama model (existing functionality)\n   - Test with OpenAI model configuration if available\n   - Test with Anthropic model configuration if available\n   - Verify UI correctly filters model selection based on capabilities\n\n4. **Manual testing scenarios**:\n   - Configure multiple providers in model_configurations\n   - Attempt to set each as tool-calling model via API\n   - Execute high-level tools and verify correct provider is used\n   - Confirm error messages are clear when selecting unsupported provider\n\n5. **Regression testing**:\n   - Ensure existing Ollama tool calling still works correctly\n   - Verify no breaking changes to high-level tool execution\n   - Test that email processing workflows continue to function",
        "status": "pending",
        "dependencies": [],
        "priority": "high",
        "subtasks": []
      },
      {
        "id": "41",
        "title": "Extend email_drafter to support all configured providers",
        "description": "Modify email_drafter.rs to use the provider adapter abstraction consistently so any configured provider (not just Ollama and OpenAI) works for email drafting, preventing runtime errors when users select other providers in the UI.",
        "details": "Fix the provider limitation in src/dashboard/services/ai/email_drafter.rs where only 'ollama' and 'openai' providers are hardcoded:\n\n1. **Analyze current implementation**:\n   - Review the existing draft_reply() and draft_email() methods that currently only support Ollama and OpenAI\n   - Identify where provider-specific logic is hardcoded (likely in a match statement or if/else chain)\n   - Check how the provider adapter abstraction is used in other services (e.g., agent_executor.rs after Task 40)\n\n2. **Refactor to use provider adapter abstraction**:\n   ```rust\n   // Instead of hardcoding provider logic:\n   match provider_name.as_str() {\n       \"ollama\" => { /* Ollama-specific code */ }\n       \"openai\" => { /* OpenAI-specific code */ }\n       _ => return Err(\"Unsupported provider\")\n   }\n   \n   // Use the provider adapter pattern:\n   let provider = self.provider_manager.get_provider(&provider_name)?;\n   let response = provider.complete(CompletionRequest {\n       model: model_name,\n       messages: vec![\n           Message::system(\"You are an email drafting assistant...\"),\n           Message::user(&prompt)\n       ],\n       temperature: Some(0.7),\n       max_tokens: Some(1000),\n       stream: false,\n   }).await?;\n   ```\n\n3. **Update EmailDrafter struct**:\n   - Add provider_manager: Arc<ProviderManager> field\n   - Remove any provider-specific client fields (ollama_client, openai_client, etc.)\n   - Update the constructor to accept ProviderManager\n\n4. **Modify draft_reply() method**:\n   - Remove provider-specific branching logic\n   - Use provider_manager.get_provider() to get the appropriate adapter\n   - Build a unified CompletionRequest that works with all providers\n   - Handle the response uniformly regardless of provider\n\n5. **Modify draft_email() method**:\n   - Apply the same refactoring as draft_reply()\n   - Ensure consistent error handling across all providers\n\n6. **Update error handling**:\n   - Replace provider-specific error types with a generic error type\n   - Ensure meaningful error messages when a provider fails\n   - Add logging for debugging provider issues\n\n7. **Consider provider capabilities**:\n   - Some providers may have different token limits or features\n   - Use the provider adapter's capabilities to adjust request parameters\n   - Gracefully degrade functionality if a provider doesn't support certain features",
        "testStrategy": "Verify multi-provider email drafting support with comprehensive testing:\n\n1. **Unit tests for EmailDrafter**:\n   - Mock ProviderManager to return different provider adapters\n   - Test draft_reply() with mocked Ollama, OpenAI, Anthropic, LM Studio, and llama.cpp providers\n   - Test draft_email() with all supported providers\n   - Verify error handling when provider_manager.get_provider() fails\n   - Test with providers that have different capabilities/limitations\n\n2. **Integration tests with real providers**:\n   - Set up test configurations for multiple providers in ai_model_configurations\n   - Test actual email drafting with each configured provider\n   - Verify response quality and format consistency across providers\n   - Test error scenarios (invalid API keys, network failures, etc.)\n\n3. **Manual testing through UI**:\n   - Configure multiple providers in the system\n   - Select each provider in the Email Assistant UI\n   - Draft replies and new emails with each provider\n   - Verify no runtime errors occur regardless of selected provider\n   - Test switching between providers during a session\n\n4. **Performance testing**:\n   - Compare response times across different providers\n   - Ensure no performance regression from the refactoring\n   - Test concurrent drafting requests with different providers\n\n5. **Regression testing**:\n   - Ensure Ollama and OpenAI (previously working providers) still function correctly\n   - Verify email quality hasn't degraded with the abstraction\n   - Test edge cases like very long emails or special formatting",
        "status": "pending",
        "dependencies": [],
        "priority": "high",
        "subtasks": []
      },
      {
        "id": "42",
        "title": "Add integration tests for AI subsystem configuration",
        "description": "Create comprehensive integration tests that verify sampler configuration persistence, provider functionality, and model selection across service restarts to prevent configuration-related bugs.",
        "details": "Create integration tests in tests/ai_configuration_integration.rs that verify the entire AI configuration subsystem works correctly:\n\n1. **Test Sampler Settings Application**:\n   ```rust\n   #[tokio::test]\n   async fn test_sampler_config_applied_to_requests() {\n       // Setup test database with custom sampler config\n       let sampler_config = SamplerConfig {\n           provider: \"ollama\",\n           model_name: \"llama3.3:70b\",\n           temperature: 0.3,\n           top_p: 0.95,\n           min_p: 0.05,\n           repeat_penalty: 1.2,\n           num_ctx: 4096,\n           // ... other settings\n       };\n       \n       // Save to database\n       sampler_service.upsert_config(sampler_config).await?;\n       \n       // Create mock Ollama server that captures requests\n       let mock_server = MockServer::start().await;\n       Mock::given(method(\"POST\"))\n           .and(path(\"/api/chat\"))\n           .respond_with(ResponseTemplate::new(200)\n               .set_body_json(json!({\"response\": \"test\"})))\n           .mount(&mock_server)\n           .await;\n       \n       // Make AI request through the system\n       let ai_service = create_ai_service_with_url(&mock_server.uri());\n       ai_service.complete(/* request */).await?;\n       \n       // Verify the request included our sampler settings\n       let received_requests = mock_server.received_requests().await;\n       let request_body: Value = serde_json::from_slice(&received_requests[0].body)?;\n       \n       assert_eq!(request_body[\"temperature\"], 0.3);\n       assert_eq!(request_body[\"top_p\"], 0.95);\n       assert_eq!(request_body[\"min_p\"], 0.05);\n       assert_eq!(request_body[\"repeat_penalty\"], 1.2);\n       assert_eq!(request_body[\"num_ctx\"], 4096);\n   }\n   ```\n\n2. **Test All Providers Can Process Requests**:\n   ```rust\n   #[tokio::test]\n   async fn test_all_ui_providers_functional() {\n       // Get list of providers shown in UI\n       let ui_providers = vec![\"ollama\", \"llamacpp\", \"lmstudio\", \"openai\", \"anthropic\"];\n       \n       for provider in ui_providers {\n           // Configure model for this provider\n           let model_config = ModelConfiguration {\n               role: \"tool_calling\",\n               provider: provider.to_string(),\n               model_name: get_test_model_for_provider(provider),\n               // ...\n           };\n           \n           model_service.set_model_config(model_config).await?;\n           \n           // Create mock server for provider\n           let mock = create_provider_mock(provider).await;\n           \n           // Attempt to make a request\n           let result = ai_service.complete(CompletionRequest {\n               prompt: \"Test prompt\",\n               // ...\n           }).await;\n           \n           // Verify request was successful\n           assert!(result.is_ok(), \"Provider {} failed to process request: {:?}\", \n                   provider, result.err());\n           \n           // Verify correct endpoint was called\n           mock.assert();\n       }\n   }\n   ```\n\n3. **Test Configuration Persistence Across Restarts**:\n   ```rust\n   #[tokio::test]\n   async fn test_config_persists_across_service_restart() {\n       // Set up initial configurations\n       let tool_model = ModelConfiguration {\n           role: \"tool_calling\",\n           provider: \"ollama\",\n           model_name: \"qwen2.5:7b\",\n       };\n       let draft_model = ModelConfiguration {\n           role: \"drafting\", \n           provider: \"lmstudio\",\n           model_name: \"llama3.3:70b\",\n       };\n       \n       // Save configurations\n       model_service.set_model_config(tool_model.clone()).await?;\n       model_service.set_model_config(draft_model.clone()).await?;\n       \n       // Save sampler configs\n       let sampler_config = SamplerConfig {\n           provider: \"ollama\",\n           model_name: \"qwen2.5:7b\",\n           temperature: 0.5,\n           think_mode: true,\n           // ...\n       };\n       sampler_service.upsert_config(sampler_config.clone()).await?;\n       \n       // Simulate service restart by dropping and recreating services\n       drop(model_service);\n       drop(sampler_service);\n       drop(ai_service);\n       \n       // Recreate services (simulating restart)\n       let model_service = ModelConfigService::new(db_pool.clone());\n       let sampler_service = SamplerConfigService::new(db_pool.clone());\n       let ai_service = create_ai_service(model_service.clone(), sampler_service.clone());\n       \n       // Verify configurations are still present\n       let loaded_tool_model = model_service.get_model_config(\"tool_calling\").await?;\n       assert_eq!(loaded_tool_model.provider, \"ollama\");\n       assert_eq!(loaded_tool_model.model_name, \"qwen2.5:7b\");\n       \n       let loaded_draft_model = model_service.get_model_config(\"drafting\").await?;\n       assert_eq!(loaded_draft_model.provider, \"lmstudio\");\n       assert_eq!(loaded_draft_model.model_name, \"llama3.3:70b\");\n       \n       let loaded_sampler = sampler_service.get_config(\"ollama\", \"qwen2.5:7b\").await?;\n       assert_eq!(loaded_sampler.temperature, 0.5);\n       assert_eq!(loaded_sampler.think_mode, true);\n   }\n   ```\n\n4. **Test Model Selection Respected in AI Calls**:\n   ```rust\n   #[tokio::test]\n   async fn test_model_selection_respected() {\n       // Configure specific models for each role\n       model_service.set_model_config(ModelConfiguration {\n           role: \"tool_calling\",\n           provider: \"ollama\",\n           model_name: \"mistral:7b\",\n       }).await?;\n       \n       model_service.set_model_config(ModelConfiguration {\n           role: \"drafting\",\n           provider: \"ollama\", \n           model_name: \"llama3.3:70b\",\n       }).await?;\n       \n       // Set up mocks that verify correct model is used\n       let tool_mock = Mock::given(method(\"POST\"))\n           .and(path(\"/api/chat\"))\n           .and(body_json_schema(json!({\n               \"model\": {\"const\": \"mistral:7b\"}\n           })))\n           .respond_with(ResponseTemplate::new(200))\n           .expect(1)\n           .mount(&mock_server)\n           .await;\n       \n       let draft_mock = Mock::given(method(\"POST\"))\n           .and(path(\"/api/chat\"))\n           .and(body_json_schema(json!({\n               \"model\": {\"const\": \"llama3.3:70b\"}\n           })))\n           .respond_with(ResponseTemplate::new(200))\n           .expect(1)\n           .mount(&mock_server)\n           .await;\n       \n       // Make tool calling request\n       tool_executor.execute(\"List my emails\").await?;\n       \n       // Make drafting request\n       email_drafter.draft_email(\"Write a test email\").await?;\n       \n       // Mocks will verify correct models were used\n   }\n   ```\n\n5. **Test Edge Cases and Error Scenarios**:\n   ```rust\n   #[tokio::test]\n   async fn test_missing_provider_configuration() {\n       // Remove all configurations\n       sqlx::query!(\"DELETE FROM ai_model_configurations\").execute(&db_pool).await?;\n       \n       // Attempt to use AI service - should use defaults or fail gracefully\n       let result = ai_service.complete(/* request */).await;\n       \n       // Verify appropriate error or default behavior\n       match result {\n           Ok(_) => {\n               // Should have used default configuration\n               let config = model_service.get_model_config(\"tool_calling\").await?;\n               assert_eq!(config.model_name, \"qwen2.5:7b\"); // Default from migration\n           },\n           Err(e) => {\n               // Should be a clear configuration error\n               assert!(e.to_string().contains(\"configuration\"));\n           }\n       }\n   }\n   ```\n\n6. **Test MCP Tool Integration**:\n   ```rust\n   #[tokio::test]\n   async fn test_mcp_tools_use_configured_models() {\n       // Configure models via MCP tools\n       let set_tool_result = mcp_handler.handle_tool_call(\n           \"set_tool_calling_model\",\n           json!({\"provider\": \"ollama\", \"model\": \"qwen2.5:7b\"})\n       ).await?;\n       \n       let set_draft_result = mcp_handler.handle_tool_call(\n           \"set_drafting_model\",\n           json!({\"provider\": \"lmstudio\", \"model\": \"llama3.3:70b\"})\n       ).await?;\n       \n       // Verify configurations were saved\n       let get_config_result = mcp_handler.handle_tool_call(\n           \"get_model_configurations\",\n           json!({})\n       ).await?;\n       \n       let configs: Vec<ModelConfiguration> = serde_json::from_value(get_config_result)?;\n       assert_eq!(configs.len(), 2);\n       \n       // Process email with configured models\n       let process_result = mcp_handler.handle_tool_call(\n           \"process_email_instructions\",\n           json!({\"instruction\": \"Reply to the latest email\"})\n       ).await?;\n       \n       // Verify correct models were used (check mock server logs)\n   }\n   ```",
        "testStrategy": "Run the integration test suite with the following verification steps:\n\n1. **Environment Setup**:\n   - Create a test database with all migrations applied\n   - Set up mock servers for each AI provider (Ollama, LM Studio, etc.)\n   - Configure test-specific environment variables for provider URLs\n\n2. **Run Individual Test Categories**:\n   ```bash\n   # Test sampler configuration application\n   cargo test test_sampler_config_applied_to_requests -- --nocapture\n   \n   # Test all providers\n   cargo test test_all_ui_providers_functional -- --nocapture\n   \n   # Test persistence\n   cargo test test_config_persists_across_service_restart -- --nocapture\n   \n   # Test model selection\n   cargo test test_model_selection_respected -- --nocapture\n   ```\n\n3. **Verify Mock Server Interactions**:\n   - Check that each test's mock server received the expected requests\n   - Verify request bodies contain correct model names and sampler parameters\n   - Ensure no unexpected API calls were made\n\n4. **Database State Verification**:\n   - After each test, query the database to verify configurations are correctly stored\n   - Check both ai_model_configurations and ai_sampler_configs tables\n   - Verify foreign key constraints are maintained\n\n5. **Error Scenario Testing**:\n   - Disconnect mock servers to test connection failures\n   - Corrupt database entries to test validation\n   - Remove configurations to test default fallbacks\n\n6. **Performance Testing**:\n   - Run tests with timing to ensure configuration lookups don't add significant latency\n   - Test with multiple concurrent requests to verify thread safety\n\n7. **Full Integration Test**:\n   ```bash\n   cargo test --test ai_configuration_integration -- --test-threads=1\n   ```\n   Run all tests sequentially to avoid database conflicts",
        "status": "pending",
        "dependencies": [],
        "priority": "medium",
        "subtasks": []
      }
    ],
    "metadata": {
      "version": "1.0.0",
      "lastModified": "2026-01-25T11:39:50.376Z",
      "taskCount": 42,
      "completedCount": 38,
      "tags": [
        "master"
      ]
    }
  }
}